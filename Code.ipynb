{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "CS 615 Final Project\n",
        "Submission Date: 3/20/2025\n",
        "William Lien, Shawn Thomas, Shobhit Dixit, Rutvij Upadhyay\n",
        "\n",
        "This notebook hosts 4 seperate CNN models. For any model usage, run the PARAMETERS and LOAD DATASET sections.\n",
        "\n",
        "While a consolidated framework would have been more convienent, setbacks in the standardization lead to each model hosting a different framework depending on the needs.\n",
        "\n",
        "The next four sections host different models (base, multi-kernel, RGB, and multi-CNN layer), alongside the functions to load images in the correct color settings, and framework specifically to run that model. These should at the time of submission hold the final loss and accuracy graphs for each run.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nBFz60USJMqa"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMFqrJrFBdUp"
      },
      "source": [
        "#Parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfVhT3-RAHZG"
      },
      "source": [
        "##Image Size Parameters\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "KzQZMbZRAG6e"
      },
      "outputs": [],
      "source": [
        "H = 80 # @param {\"type\":\"integer\"}\n",
        "W = 80 # @param {\"type\":\"integer\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9QfiJkb80to"
      },
      "source": [
        "##Dataset Parameters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "xYhKtBKD77oH"
      },
      "outputs": [],
      "source": [
        "trainTrue = 88 # @param {\"type\":\"integer\"}\n",
        "trainFalse = 112 # @param {\"type\":\"integer\"}\n",
        "testTrue = 25 # @param {\"type\":\"integer\"}\n",
        "testFalse = 25 # @param {\"type\":\"integer\"}\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0sLi--ZT9jTG"
      },
      "source": [
        "##Learning Rate Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "bnIfir-G9hXz"
      },
      "outputs": [],
      "source": [
        "FCLearn = .0025 # @param {\"type\":\"number\"}\n",
        "KLearn = .0025 # @param {\"type\":\"number\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFjp5Ilv-dcw"
      },
      "source": [
        "##Kernel,Max-pool, FC Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "E1PBD46g_Alc"
      },
      "outputs": [],
      "source": [
        "kSize = 5 # @param {\"type\":\"integer\"}\n",
        "numKernels = 3 # @param {\"type\":\"integer\"}\n",
        "mpSize = 2 # @param {\"type\":\"integer\"}\n",
        "mpStride = 2 # @param {\"type\":\"integer\"}\n",
        "weightOut = 1 # @param {\"type\":\"integer\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7ogmhMqCbSW"
      },
      "source": [
        "##Epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "QWEQx8UtCRAR"
      },
      "outputs": [],
      "source": [
        "epochs = 250 # @param {\"type\":\"integer\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkHOriCZceeR"
      },
      "source": [
        "#Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bOIeng_2cf6Z",
        "outputId": "9d9ba464-9e4a-4de1-db92-7d056c891ed9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/tusharpadhy/deepfake-dataset?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5.42G/5.42G [01:10<00:00, 82.9MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "import numpy as np\n",
        "import math\n",
        "from scipy import signal\n",
        "from abc import ABC, abstractmethod\n",
        "import random\n",
        "import sys\n",
        "import os\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "fpath = kagglehub.dataset_download(\"tusharpadhy/deepfake-dataset\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Zd0R8k_iMjr"
      },
      "source": [
        "#Base Model - Greyscale, 1 CNN, 1 Kernel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydtQCwDNajMJ"
      },
      "source": [
        "##Framework"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wAwkudUEae7L"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "from scipy import signal\n",
        "from abc import ABC, abstractmethod\n",
        "import random\n",
        "import sys\n",
        "import os\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "\n",
        "##########BASE CLASS###########\n",
        "class Layer(ABC):\n",
        "  def __init__(self):\n",
        "    self.__prevIn = []\n",
        "    self.__prevOut = []\n",
        "\n",
        "  def setPrevIn(self,dataIn):\n",
        "    self.__prevIn = dataIn\n",
        "\n",
        "  def setPrevOut(self, out):\n",
        "    self.__prevOut = out\n",
        "\n",
        "  def getPrevIn(self):\n",
        "    return self.__prevIn\n",
        "\n",
        "  def getPrevOut(self):\n",
        "    return self.__prevOut\n",
        "\n",
        "  \"\"\"\n",
        "  def backward(self, gradIn):\n",
        "    sg = self.gradient()\n",
        "    gradOut = np.zeros((gradIn.shape[0],sg.shape[2]))\n",
        "\n",
        "    for n in range(gradIn.shape[0]):\n",
        "        gradOut[n] = np.atleast_2d(gradIn[n])@sg[n]\n",
        "    return gradOut\n",
        "  \"\"\"\n",
        "  def backward(self,gradIn):\n",
        "    #from lecture slides:\n",
        "\n",
        "    #sg = self.gradient()\n",
        "    #gradOut = np.zeros((gradIn.shape[0],sg.shape[1]))\n",
        "    #for i in range(gradIn.shape[0]):\n",
        "    #  gradOut[i] = np.atleast_2d(gradIn[i])@np.atleast_2d(sg[i])\n",
        "    #  return gradOut\n",
        "\n",
        "\n",
        "    sg = self.gradient()\n",
        "\n",
        "    if(sg.ndim == 3):  #tensor coming back\n",
        "        gradOut = np.zeros((gradIn.shape[0],sg.shape[2]))\n",
        "        for i in range(gradIn.shape[0]):\n",
        "            gradOut[i] = np.atleast_2d(gradIn[i])@np.atleast_2d(sg[i])\n",
        "    else:\n",
        "        gradOut = np.atleast_2d(gradIn)@sg\n",
        "\n",
        "    return gradOut\n",
        "\n",
        "  @abstractmethod\n",
        "  def forward(self,dataIn):\n",
        "    pass\n",
        "\n",
        "  @abstractmethod\n",
        "  def gradient(self):\n",
        "    pass\n",
        "\n",
        "##########BASE CLASS###########\n",
        "class Objective(ABC):\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def eval(Y,Yhat):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def gradient(Y,Yhat):\n",
        "        pass\n",
        "\n",
        "\n",
        "#\\/\\/\\/\\/CONVOLUTION\\/\\/\\/\\/\n",
        "class ConvolutionalLayer(Layer):\n",
        "\n",
        "    '''Constructor (init)- Your constructor should take in a single\n",
        "    explicit parameter, the kernel size.\n",
        "    We will assume the kernel is square, of an odd size,\n",
        "    and that will have a single kernel '''\n",
        "\n",
        "    def __init__(self, kernelSize):\n",
        "        super().__init__()\n",
        "        #is zeros right?\n",
        "        #check for odd number size?\n",
        "        a = -1e-1\n",
        "        b = 1e-1\n",
        "        self.kernel = np.random.uniform(a, b, (kernelSize, kernelSize))\n",
        "\n",
        "\n",
        "    def setKernels(self,K):\n",
        "        '''- This method should take a matrix (or tensor) as a parameter\n",
        "        and set the weights of the kernel(s) to it. '''\n",
        "\n",
        "        self.kernel = K\n",
        "\n",
        "    def getKernels(self):\n",
        "        #This returns the kernel(s).\n",
        "        return self.kernel\n",
        "\n",
        "    @staticmethod\n",
        "    def crossCorrelate2D(K,N):\n",
        "        '''This static method should take two matrices as parameters,\n",
        "        and return their cross-correlation. You do not need to support padding.'''\n",
        "        N_height, N_width = N.shape\n",
        "        K_height, K_width = K.shape\n",
        "\n",
        "        result_height = N_height - K_height + 1\n",
        "        result_width = N_width - K_width + 1\n",
        "\n",
        "        result = np.zeros((result_height, result_width))\n",
        "\n",
        "        for i in range(result_height):\n",
        "            for j in range(result_width):\n",
        "                submatrix = N[i: i+K_height, j: j+K_width]\n",
        "                result[i, j] = np.sum(submatrix * K)\n",
        "\n",
        "\n",
        "        return result\n",
        "\n",
        "\n",
        "    def forward(self,X):\n",
        "        '''This method should take a tensor of incoming data (N × H × W),\n",
        "        set prevIn with it, compute the cross-correlation of\n",
        "        each observations with the kernel,\n",
        "        store this result with its parent class (prevOut)\n",
        "        and return it (this output should also be a tensor). '''\n",
        "        self.setPrevIn(X)\n",
        "        N,H,W = X.shape\n",
        "        outputs = []\n",
        "\n",
        "        for i in range(N):\n",
        "\n",
        "            result = ConvolutionalLayer.crossCorrelate2D(self.kernel,X[i])\n",
        "\n",
        "            outputs.append(result)\n",
        "\n",
        "        result = np.stack(outputs,axis=0)\n",
        "        self.setPrevOut(result)\n",
        "        return result\n",
        "\n",
        "\n",
        "    def gradient(self):\n",
        "        pass\n",
        "\n",
        "    def backward(self):\n",
        "        pass\n",
        "\n",
        "    def updateKernels(self,backGrad,eta):\n",
        "        X = self.getPrevIn()\n",
        "        N, H, W = X.shape\n",
        "\n",
        "        kernelGrad = np.zeros(self.kernel.shape,dtype='float64')\n",
        "\n",
        "\n",
        "        for n in range(N):\n",
        "            for i in range(H - self.kernel.shape[0] + 1):\n",
        "                for j in range(W - self.kernel.shape[1] + 1):\n",
        "                    input_patch = X[n, i:i + self.kernel.shape[0], j:j + self.kernel.shape[1]]\n",
        "\n",
        "                    kernelGrad += input_patch * backGrad[n, i, j]\n",
        "\n",
        "\n",
        "        self.kernel = self.kernel -  eta * kernelGrad\n",
        "\n",
        "#\\/\\/\\/\\/Cross Entropy\\/\\/\\/\\/\n",
        "\n",
        "class CrossEntropy(Objective):\n",
        "    def eval(self,Y,Yhat):\n",
        "        constant = 10**-7\n",
        "        tmp = Y*np.log(Yhat+constant)\n",
        "        return -np.mean(np.sum(tmp,axis=1))\n",
        "\n",
        "\n",
        "\n",
        "    def gradient(self,Y,Yhat):\n",
        "        constant = 10e-7\n",
        "        return -(Y/(Yhat + constant))\n",
        "\n",
        "\n",
        "#\\/\\/\\/\\/Flatten\\/\\/\\/\\/\n",
        "class FlatteningLayer(Layer):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, X):\n",
        "        self.setPrevIn(X)\n",
        "\n",
        "        N,H,W = X.shape\n",
        "        flatten = X.reshape(N,-1,order=\"F\")\n",
        "\n",
        "        self.setPrevOut(flatten)\n",
        "        return flatten\n",
        "\n",
        "    def gradient(self):\n",
        "        pass\n",
        "\n",
        "    def backward(self,gradIn):\n",
        "        #return original dimensions\n",
        "        N,H,W = self.getPrevIn().shape\n",
        "\n",
        "        gradOut = gradIn.reshape(N,H,W, order=\"F\")\n",
        "\n",
        "        return gradOut\n",
        "\n",
        "    def backward2(self,gradIn):\n",
        "        #return original dimensions\n",
        "        #copy -- all other functions for Q2 have a backward 2 method, this makes it easier without messing with the test script\n",
        "        N,H,W = self.getPrevIn().shape\n",
        "\n",
        "        gradOut = gradIn.reshape(N,H,W, order=\"F\")\n",
        "\n",
        "        return gradOut\n",
        "\n",
        "\n",
        "#\\/\\/\\/\\/FullyConnected\\/\\/\\/\\/\n",
        "class FullyConnectedLayer(Layer):\n",
        "     #Input : sizeIn , the number of features of data coming in\n",
        "    # #Input : sizeOut , the number of features for the data coming out.\n",
        "    # #Output: None\n",
        "\n",
        "    def __init__(self, sizeIn, sizeOut,useBias=0):\n",
        "        super().__init__()\n",
        "        #useBias needed for test, unsure where it fits in this setup\n",
        "\n",
        "        #initialize weight and bias\n",
        "        #Xaiver\n",
        "        a =  -(6/(sizeIn+sizeOut))**(1/2)\n",
        "        b = (6/(sizeIn+sizeOut))**(1/2)\n",
        "        self.W = np.random.uniform(a, b, (sizeIn, sizeOut))\n",
        "        self.B = np.random.uniform(a, b, (1, sizeOut))\n",
        "\n",
        "     #  #Input : None\n",
        "    # #Output: The (sizeIn by sizeOut) weight matrix.\n",
        "    def getWeights(self):\n",
        "        return self.W\n",
        "\n",
        "    # #Input : The (sizeIn by sizeOut) weight matrix.\n",
        "    # #Output: None\n",
        "    def setWeights(self, weights):\n",
        "        self.W = weights\n",
        "\n",
        "\n",
        "     # #Input : None\n",
        "    # #Output: The (1 by sizeOut) bias vector\n",
        "    def getBiases(self):\n",
        "        return self.B\n",
        "\n",
        "     # #Input : The (1 by sizeOut) bias vector\n",
        "    # #Output: None\n",
        "    def setBiases(self, biases):\n",
        "        self.B = biases\n",
        "\n",
        "     # #Input : dataIn , a (1 by D) data matrix\n",
        "    # #Output: A (1 by K) data matrix\n",
        "    def forward(self, dataIn):\n",
        "\n",
        "        self.setPrevIn(dataIn)\n",
        "        Y = np.dot(dataIn, self.W) + self.B\n",
        "        self.setPrevOut(Y)\n",
        "        return Y\n",
        "\n",
        "    # #We’ ll worry about this later ...\n",
        "    def gradient( self ):\n",
        "        #deriv = wT\n",
        "        return self.W.T\n",
        "\n",
        "    def updateWeights(self,gradIn,eta):\n",
        "        #from lecture slide\n",
        "        #bias graidnet - average of sum of gradIn (avg hadmaradnd product of bias and identity)\n",
        "        #weight - input transpose @ gradIn avged over gradient shape.\n",
        "        #both updates of X = X-learningrate*gradient calc\n",
        "        getW = self.getWeights()\n",
        "        dJdb = np.sum(gradIn, axis=0)/gradIn.shape[0]\n",
        "        dJdW = (self.getPrevIn().T @ gradIn)/gradIn.shape[0]\n",
        "\n",
        "\n",
        "        self.W = self.W -  eta*dJdW\n",
        "        self.B = self.B - eta*dJdb\n",
        "\n",
        "    def backward2(self,gradIn):\n",
        "\n",
        "        gradOut = np.dot(gradIn, self.gradient())\n",
        "        return gradOut\n",
        "\n",
        "#\\/\\/\\/\\/Input\\/\\/\\/\\/\n",
        "class InputLayer(Layer):\n",
        "    #Input : dataIn , an (N by D) matrix\n",
        "    # #Output: None\n",
        "\n",
        "    def __init__( self , dataIn,z_score=0 ):\n",
        "        #take the mean and standard deviation of each column\n",
        "        if z_score != 0:\n",
        "            self.meanX = np.mean(dataIn,axis=0)\n",
        "            self.stdX = np.std(dataIn,axis=0,ddof=1)\n",
        "            self.stdX[self.stdX == 0] = 1\n",
        "        else:\n",
        "           self.meanX = 0\n",
        "           self.stdX = 1\n",
        "\n",
        "    # #Input : dataIn , a (1 by D) matrix\n",
        "    # #Output: A (1 by D) matrix\n",
        "    def forward( self , dataIn ):\n",
        "        #review unsure of the 1 x D and the stored attributes\n",
        "        self.setPrevIn(dataIn)\n",
        "        zscored = (dataIn-self.meanX)/self.stdX\n",
        "        self.setPrevOut(zscored)\n",
        "\n",
        "        return zscored\n",
        "\n",
        "    # #We’ ll worry about this later ...\n",
        "    def gradient( self ):\n",
        "     pass\n",
        "\n",
        "#\\/\\/\\/\\/Linear\\/\\/\\/\\/\n",
        "\n",
        "class LinearLayer(Layer):\n",
        "    #Input : None\n",
        "    # #Output: None\n",
        "    def __init__ ( self ):\n",
        "        super().__init__()\n",
        "\n",
        "\n",
        "    #Input : dataIn , a (1 by K) matrix\n",
        "    # #Output: A (1 by K) matrix\n",
        "    def forward( self , dataIn ):\n",
        "        self.setPrevIn(dataIn)\n",
        "        self.setPrevOut(dataIn)\n",
        "        return dataIn\n",
        "\n",
        "\n",
        "\n",
        "    # #We’ ll worry about this later ...\n",
        "    def gradient( self ):\n",
        "        #identity matrix\n",
        "        m,n = self.getPrevIn().shape\n",
        "        return np.eye(n)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#\\/\\/\\/\\/Sigmoid\\/\\/\\/\\/\n",
        "class LogisticSigmoidLayer(Layer):\n",
        "    #Input : None\n",
        "    # #Output: None\n",
        "    def __init__ ( self ):\n",
        "        super().__init__()\n",
        "\n",
        "    #Input : dataIn , a (1 by K) matrix\n",
        "    # #Output: A (1 by K) matrix\n",
        "    def forward( self , dataIn ):\n",
        "        self.setPrevIn(dataIn)\n",
        "\n",
        "        clippedData = np.clip(dataIn, -500, 500)\n",
        "\n",
        "        Y = 1/(1+np.exp(-clippedData))\n",
        "        self.setPrevOut(Y)\n",
        "        return Y\n",
        "\n",
        "\n",
        "    # #We’ ll worry about this later ...\n",
        "    def gradient( self ):\n",
        "        m, n = self.getPrevIn().shape\n",
        "\n",
        "        # Calculate sigmoid derivative for each element\n",
        "        G = self.getPrevOut() * (1 - self.getPrevOut())\n",
        "\n",
        "        # Initialize output array with proper shape (m,n,n)\n",
        "        gradient_matrices = np.zeros((m, n, n))\n",
        "\n",
        "        # For each sample, create a diagonal matrix\n",
        "        for i in range(m):\n",
        "            gradient_matrices[i] = np.diag(G[i])\n",
        "\n",
        "        return gradient_matrices\n",
        "\n",
        "    def gradient2(self):\n",
        "        G = self.getPrevOut() * (1 - self.getPrevOut())\n",
        "        return G\n",
        "\n",
        "    def backward2(self,gradIn):\n",
        "        gradOut = gradIn*self.gradient2()\n",
        "        return gradOut\n",
        "\n",
        "#\\/\\/\\/\\/LogLoss\\/\\/\\/\\/\n",
        "class LogLoss(Objective):\n",
        "    def eval(self,Y,Yhat):\n",
        "        constant = 10**-7\n",
        "        return np.mean(-((Y*np.log(Yhat+constant))+(1-Y)*np.log(1-Yhat+constant)))\n",
        "\n",
        "    def gradient(self,Y,Yhat):\n",
        "        constant = 10**-7\n",
        "        #Y = np.expand_dims(Y, axis=1) # placeholder -- broadcasting issue\n",
        "\n",
        "        return np.atleast_2d((1-Y)/(1-Yhat+constant)-Y/(Yhat+constant))\n",
        "\n",
        "#\\/\\/\\/\\/MaxPoolLayer\\/\\/\\/\\/\n",
        "class MaxPoolLayer(Layer):\n",
        "    def __init__ ( self,size,stride ):\n",
        "        super().__init__()\n",
        "        self.size = size\n",
        "        self.stride = stride\n",
        "        self.indices = None\n",
        "\n",
        "    def forward(self,X):\n",
        "        self.setPrevIn(X)\n",
        "        N, H, W = X.shape\n",
        "\n",
        "        out_height = (H - self.size) // self.stride + 1\n",
        "        out_width = (W - self.size) // self.stride + 1\n",
        "\n",
        "        output = np.zeros((N, out_height, out_width))\n",
        "        self.indices = np.zeros((N, out_height, out_width), dtype=int)  # To store indices of max values\n",
        "\n",
        "\n",
        "        for n in range(N):\n",
        "            for i in range(out_height):\n",
        "                for j in range(out_width):\n",
        "\n",
        "                    start_i = i * self.stride\n",
        "                    start_j = j * self.stride\n",
        "                    end_i = start_i + self.size\n",
        "                    end_j = start_j + self.size\n",
        "\n",
        "\n",
        "                    window = X[n, start_i:end_i, start_j:end_j]\n",
        "\n",
        "\n",
        "                    maxVal = np.max(window)\n",
        "                    maxIdx = np.argmax(window)\n",
        "\n",
        "\n",
        "                    output[n, i, j] = maxVal\n",
        "\n",
        "                    self.indices[n, i, j] = maxIdx\n",
        "\n",
        "        self.setPrevOut(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def gradient(self):\n",
        "        pass\n",
        "\n",
        "    def backward(self,gradIn):\n",
        "        N, H, W = self.getPrevIn().shape\n",
        "\n",
        "        gradOut = np.zeros((N,H,W))\n",
        "\n",
        "        for n in range(N):\n",
        "            for i in range(gradIn.shape[1]):\n",
        "                for j in range(gradIn.shape[2]):\n",
        "                    max_idx = self.indices[n, i, j]\n",
        "\n",
        "                    # Converting flattened index back\n",
        "                    max_i, max_j = np.unravel_index(max_idx, (self.size, self.size))\n",
        "\n",
        "                    start_i = i * self.stride\n",
        "                    start_j = j * self.stride\n",
        "\n",
        "                    global_i = start_i + max_i\n",
        "                    global_j = start_j + max_j\n",
        "\n",
        "                    gradOut[n, global_i, global_j] += gradIn[n, i, j]\n",
        "\n",
        "        return gradOut\n",
        "\n",
        "\n",
        "    def backward2(self,gradIn):\n",
        "        #copy -- all other functions for Q2 have a backward 2 method,\n",
        "        # this makes it easier without messing with the test script\n",
        "\n",
        "        N, H, W = self.getPrevIn().shape\n",
        "\n",
        "        gradOut = np.zeros((N,H,W))\n",
        "\n",
        "        for n in range(N):\n",
        "            for i in range(gradIn.shape[1]):\n",
        "                for j in range(gradIn.shape[2]):\n",
        "                    max_idx = self.indices[n, i, j]\n",
        "\n",
        "                    # Converting flattened index back\n",
        "                    max_i, max_j = np.unravel_index(max_idx, (self.size, self.size))\n",
        "\n",
        "                    start_i = i * self.stride\n",
        "                    start_j = j * self.stride\n",
        "\n",
        "                    global_i = start_i + max_i\n",
        "                    global_j = start_j + max_j\n",
        "\n",
        "                    gradOut[n, global_i, global_j] += gradIn[n, i, j]\n",
        "\n",
        "        return gradOut\n",
        "\n",
        "#\\/\\/\\/\\/ReLU\\/\\/\\/\\/\n",
        "class ReLULayer(Layer):\n",
        "    #Input : None\n",
        "    # #Output: None\n",
        "    def __init__ ( self ):\n",
        "        super().__init__()\n",
        "\n",
        "    #Input : dataIn , a (1 by K) matrix\n",
        "    # #Output: A (1 by K) matrix\n",
        "    def forward( self , dataIn ):\n",
        "        self.setPrevIn(dataIn)\n",
        "        Y = np.maximum(0,dataIn)\n",
        "        self.setPrevOut(Y)\n",
        "        return Y\n",
        "\n",
        "\n",
        "    # #We’ ll worry about this later ...\n",
        "    def gradient( self ):\n",
        "        #near-identity, but value set to 0 if z < 0\n",
        "        m,n = self.getPrevIn().shape\n",
        "        I = np.eye(n)\n",
        "        I[I<0] = 0\n",
        "        return I\n",
        "\n",
        "    def backward2(self, gradIn):\n",
        "        # Create a mask for the derivative of ReLU (1 for positive values, 0 for non-positive)\n",
        "        gradOut = gradIn * (self.getPrevIn() > 0).astype(float)\n",
        "        return gradOut  # Return the gradient that should propagate backward\n",
        "\n",
        "#\\/\\/\\/\\/SOFTMAX\\/\\/\\/\\/\n",
        "class SoftmaxLayer(Layer):\n",
        "    #Input : None\n",
        "    # #Output: None\n",
        "    def __init__ ( self ):\n",
        "        super().__init__()\n",
        "\n",
        "    #Input : dataIn , a (1 by K) matrix\n",
        "    # #Output: A (1 by K) matrix\n",
        "    def forward( self , dataIn ):\n",
        "        #original\n",
        "        #self.setPrevIn(dataIn)\n",
        "        #Y = (np.e**dataIn/np.sum(np.e**dataIn))\n",
        "        #self.setPrevOut(Y)\n",
        "        #return Y\n",
        "\n",
        "        #Changes for (N x K) -- compute rowwise\n",
        "        self.setPrevIn(dataIn)\n",
        "\n",
        "        Y = np.exp(dataIn)  / np.sum(np.exp(dataIn) , axis=1, keepdims=True)\n",
        "\n",
        "        self.setPrevOut(Y)\n",
        "        return Y\n",
        "\n",
        "    # #We’ ll worry about this later ...\n",
        "    def gradient( self ):\n",
        "        #on diagonals = gj(z)(1-gj(z))\n",
        "        #elsewhere = -gi(z)*gj(z)\n",
        "        #simplified = gi(z)((i==j)-gj(z))\n",
        "        #evenmore = diag(g(z))-g(z)T * g(z)\n",
        "\n",
        "        po = self.getPrevOut()\n",
        "\n",
        "\n",
        "        batch = po.shape[0]\n",
        "        K = po.shape[1]\n",
        "\n",
        "        grad = np.zeros((batch, K, K))\n",
        "\n",
        "        for b in range(batch):\n",
        "            for i in range(K):\n",
        "                for j in range(K):\n",
        "                    if i == j:\n",
        "                        grad[b, i, j] = po[b, i] * (1 - po[b, i])  # Diagonal: gi(z) * (1 - gi(z))\n",
        "                    else:\n",
        "                        grad[b, i, j] = -po[b, i] * po[b, j]  # Off-diagonal: -gi(z) * gj(z)\n",
        "\n",
        "        return grad\n",
        "\n",
        "class SquaredError(Objective):\n",
        "    def eval(self,Y,Yhat):\n",
        "        return np.mean((Y-Yhat)**2)\n",
        "\n",
        "    def gradient(self,Y,Yhat):\n",
        "        return np.atleast_2d(-2* (Y-Yhat) )\n",
        "\n",
        "    def SMAPE(self,Y,Yhat):\n",
        "        return np.mean(np.abs(Y-Yhat)/(np.abs(Y) + np.abs(Yhat)))\n",
        "\n",
        "#\\/\\/\\/\\/TANH\\/\\/\\/\\/\n",
        "class TanhLayer(Layer):\n",
        "    #Input : None\n",
        "    # #Output: None\n",
        "    def __init__ ( self ):\n",
        "        super().__init__()\n",
        "\n",
        "    #Input : dataIn , a (1 by K) matrix\n",
        "    # #Output: A (1 by K) matrix\n",
        "    def forward( self , dataIn ):\n",
        "        self.setPrevIn(dataIn)\n",
        "        Y = (np.e**dataIn - np.e**(-dataIn))/(np.e**(dataIn) + np.e**(-dataIn))\n",
        "        self.setPrevOut(Y)\n",
        "        return Y\n",
        "\n",
        "    # #We’ ll worry about this later ...\n",
        "    def gradient( self ):\n",
        "        #derv -- (1-gj^2(z)) on diagonal\n",
        "\n",
        "        m, n = self.getPrevIn().shape\n",
        "\n",
        "        # Calculate sigmoid derivative for each element\n",
        "        G = 1-(self.getPrevOut()**2)\n",
        "\n",
        "        # Initialize output array with proper shape (m,n,n)\n",
        "        grad_matrix = np.zeros((m, n, n))\n",
        "\n",
        "        # For each sample, create a diagonal matrix\n",
        "        for i in range(m):\n",
        "            grad_matrix[i] = np.diag(G[i,:])\n",
        "\n",
        "        return grad_matrix\n",
        "\n",
        "    def gradient2(self):\n",
        "        # Calculate sigmoid derivative for each element\n",
        "        G = 1-(self.getPrevOut()**2)\n",
        "        return G\n",
        "\n",
        "    def backward2(self,gradIn):\n",
        "        gradOut = gradIn*self.gradient2()\n",
        "        return gradOut\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNP3URqVjK42"
      },
      "source": [
        "##Load Greyscale"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HmJ5LolN6LGP"
      },
      "outputs": [],
      "source": [
        "def loadImages(fpath, folder, label, max_images):\n",
        "    \"\"\"\n",
        "    Loads images from a given directory and assigns them a label.\n",
        "    \"\"\"\n",
        "    images = []\n",
        "    labels = []\n",
        "\n",
        "    folder_path = os.path.join(fpath, folder)\n",
        "    for subfolder in ['Fake', 'Real']:\n",
        "        spath = os.path.join(folder_path, subfolder)\n",
        "        for file in os.listdir(spath):\n",
        "            img_path = os.path.join(spath, file)\n",
        "            image = Image.open(img_path).convert(\"L\")\n",
        "            image = image.resize((H,W))\n",
        "            image_array = ((np.array(image) )/ 255.0)\n",
        "            images.append(image_array)\n",
        "            labels.append(label)\n",
        "\n",
        "            # data set too large, setting limits\n",
        "            if len(images) >= max_images:\n",
        "                return images, labels\n",
        "\n",
        "    return images, labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LP30PDKZcGOq"
      },
      "source": [
        "##Run Base Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "aAfTa2__bhtl",
        "outputId": "b77a6c97-c520-4d8a-87ce-daf7df77f9ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Begin Training\n",
            "Current epoch:  10\n",
            "Trues:  8\n",
            "Falses:  192\n",
            "last acc:  0.55\n",
            "Current epoch:  20\n",
            "Trues:  7\n",
            "Falses:  193\n",
            "last acc:  0.555\n",
            "Current epoch:  30\n",
            "Trues:  7\n",
            "Falses:  193\n",
            "last acc:  0.555\n",
            "Current epoch:  40\n",
            "Trues:  5\n",
            "Falses:  195\n",
            "last acc:  0.555\n",
            "Current epoch:  50\n",
            "Trues:  4\n",
            "Falses:  196\n",
            "last acc:  0.56\n",
            "Current epoch:  60\n",
            "Trues:  4\n",
            "Falses:  196\n",
            "last acc:  0.56\n",
            "Current epoch:  70\n",
            "Trues:  4\n",
            "Falses:  196\n",
            "last acc:  0.56\n",
            "Current epoch:  80\n",
            "Trues:  4\n",
            "Falses:  196\n",
            "last acc:  0.56\n",
            "Current epoch:  90\n",
            "Trues:  4\n",
            "Falses:  196\n",
            "last acc:  0.56\n",
            "Last 100 Epochs:  941.868559021\n",
            "Current epoch:  100\n",
            "Trues:  4\n",
            "Falses:  196\n",
            "last acc:  0.56\n",
            "Current epoch:  110\n",
            "Trues:  4\n",
            "Falses:  196\n",
            "last acc:  0.56\n",
            "Current epoch:  120\n",
            "Trues:  4\n",
            "Falses:  196\n",
            "last acc:  0.56\n",
            "Current epoch:  130\n",
            "Trues:  4\n",
            "Falses:  196\n",
            "last acc:  0.56\n",
            "Current epoch:  140\n",
            "Trues:  2\n",
            "Falses:  198\n",
            "last acc:  0.56\n",
            "Current epoch:  150\n",
            "Trues:  2\n",
            "Falses:  198\n",
            "last acc:  0.56\n",
            "Current epoch:  160\n",
            "Trues:  2\n",
            "Falses:  198\n",
            "last acc:  0.56\n",
            "Current epoch:  170\n",
            "Trues:  2\n",
            "Falses:  198\n",
            "last acc:  0.56\n",
            "Current epoch:  180\n",
            "Trues:  2\n",
            "Falses:  198\n",
            "last acc:  0.56\n",
            "Current epoch:  190\n",
            "Trues:  2\n",
            "Falses:  198\n",
            "last acc:  0.56\n",
            "Last 100 Epochs:  942.2365743330001\n",
            "Current epoch:  200\n",
            "Trues:  2\n",
            "Falses:  198\n",
            "last acc:  0.56\n",
            "Current epoch:  210\n",
            "Trues:  2\n",
            "Falses:  198\n",
            "last acc:  0.56\n",
            "Current epoch:  220\n",
            "Trues:  2\n",
            "Falses:  198\n",
            "last acc:  0.56\n",
            "Current epoch:  230\n",
            "Trues:  2\n",
            "Falses:  198\n",
            "last acc:  0.56\n",
            "Current epoch:  240\n",
            "Trues:  2\n",
            "Falses:  198\n",
            "last acc:  0.56\n",
            "Ending Train Accuracy:  0.56\n",
            "Ending Train Loss:  0.6855078245844948\n",
            "Ending Test Accuracy:  0.5\n",
            "Ending Test Loss:  0.7011364625628179\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "\n",
        "def base():\n",
        "    #load in and resize images ( 1 of each, use the ending tags to grab all)\n",
        "    testSet = []\n",
        "    trainSet = []\n",
        "    testLabels = []\n",
        "    trainLabels = []\n",
        "\n",
        "    testFakeImg, testFakeLabels = loadImages(fpath, 'test', 0, testFalse)\n",
        "    testRealImg, testRealLabels = loadImages(fpath, 'test', 1, testTrue)\n",
        "\n",
        "    trainFakeImg, trainFakeLabels = loadImages(fpath, 'train', 0,trainFalse)\n",
        "    trainRealImg, trainRealLabels = loadImages(fpath, 'train', 1, trainTrue)\n",
        "\n",
        "    testSet.extend(testFakeImg)\n",
        "    testSet.extend(testRealImg)\n",
        "    testLabels.extend(testFakeLabels)\n",
        "    testLabels.extend(testRealLabels)\n",
        "\n",
        "    trainSet.extend(trainFakeImg)\n",
        "    trainSet.extend(trainRealImg)\n",
        "    trainLabels.extend(trainFakeLabels)\n",
        "    trainLabels.extend(trainRealLabels)\n",
        "\n",
        "    testLabels = np.array(testLabels).reshape(-1,1)\n",
        "    trainLabels = np.array(trainLabels).reshape(-1,1)\n",
        "\n",
        "    #create index of length of images for test and train\n",
        "    #then shuffle, pass shuffled indices into arrays\n",
        "    testSet = np.array(testSet)\n",
        "    trainSet = np.array(trainSet)\n",
        "    testIndicies = np.arange(testSet.shape[0])\n",
        "    trainIndicies = np.arange(trainSet.shape[0])\n",
        "\n",
        "    testShuffle = np.random.permutation(testIndicies)\n",
        "    testSet = testSet[testShuffle]\n",
        "    testLabels = testLabels[testShuffle]\n",
        "\n",
        "\n",
        "    # First convolution layer (kernel size: 5x5)\n",
        "    hConv = H - kSize + 1\n",
        "    wConv = W - kSize + 1\n",
        "\n",
        "    # First MaxPool layer (size: 2, stride: 2)\n",
        "    hPool = ((hConv - mpSize) // mpStride) + 1\n",
        "    wPool = ((wConv - mpSize) // mpStride) + 1\n",
        "\n",
        "    # Flattening layer\n",
        "    flatSize = hPool * wPool\n",
        "\n",
        "\n",
        "\n",
        "    L1 = ConvolutionalLayer(kSize) # for kernel size NxN output size: ((H - N+2P)/S) + 1\n",
        "    L2 = MaxPoolLayer(mpSize,mpStride) #output size, stride M and size DxD: ((H-D)/M)+1\n",
        "    L3 = FlatteningLayer() #HxW\n",
        "    L4 = FullyConnectedLayer(flatSize,1) # 1 FC, 1 sigmoid\n",
        "    L5 = LogisticSigmoidLayer()\n",
        "    L6 = LogLoss()\n",
        "    layers = [L1,L2,L3,L4,L5,L6]\n",
        "    loss = []\n",
        "    accuracy = []\n",
        "\n",
        "#train\n",
        "    tic = time.perf_counter()\n",
        "    print(\"Begin Training\")\n",
        "    for epoch in range(epochs):\n",
        "      if (epoch%100 == 0) and (epoch != 0):\n",
        "          toc = time.perf_counter()\n",
        "          print(\"Last 100 Epochs: \",(toc-tic))\n",
        "          tic = time.perf_counter()\n",
        "      #shuffle each run to encourage learning\n",
        "      trainShuffle = np.random.permutation(trainIndicies)\n",
        "      trainSet = trainSet[trainShuffle]\n",
        "      trainLabels = trainLabels[trainShuffle]\n",
        "      X = trainSet.copy()\n",
        "      #forward passes\n",
        "      logloss = 0\n",
        "      for layer in layers[:-1]:\n",
        "          X = layer.forward(X)\n",
        "\n",
        "\n",
        "      logloss = layers[-1].eval(trainLabels,X)\n",
        "\n",
        "      loss.append(logloss)\n",
        "\n",
        "\n",
        "      acc = np.mean((X >= 0.5).astype(int) == trainLabels)\n",
        "      accuracy.append(acc)\n",
        "\n",
        "      if (epoch%10 == 0) and (epoch != 0):\n",
        "          print(\"Current epoch: \", epoch)\n",
        "          print(\"Trues: \", np.sum((X >= 0.5)))\n",
        "          print(\"Falses: \", np.sum((X < 0.5)))\n",
        "\n",
        "          print(\"last acc: \",acc)\n",
        "\n",
        "\n",
        "      #print(loss,acc)\n",
        "      #backwards!\n",
        "      #from slide: backwards gradients and update FC weights using training set\n",
        "      grad = layers[-1].gradient(trainLabels,X)\n",
        "      for i in range(len(layers)-2,0,-1):\n",
        "\n",
        "          newgrad = layers[i].backward2(grad)\n",
        "\n",
        "          if (isinstance(layers[i],FullyConnectedLayer)):\n",
        "                  layers[i].updateWeights(grad,FCLearn)\n",
        "\n",
        "\n",
        "\n",
        "          grad= newgrad\n",
        "      layers[0].updateKernels(grad,KLearn)\n",
        "\n",
        "\n",
        "\n",
        "    print(\"Ending Train Accuracy: \",   accuracy[-1])\n",
        "    print(\"Ending Train Loss: \",loss[-1])\n",
        "\n",
        "  #run on test:\n",
        "\n",
        "    X = testSet.copy()\n",
        "    #forward passes\n",
        "    logloss = 0\n",
        "    for layer in layers[:-1]:\n",
        "        X = layer.forward(X)\n",
        "\n",
        "\n",
        "    logloss = layers[-1].eval(testLabels,X)\n",
        "\n",
        "\n",
        "    acc = np.sum((X >= 0.5) == testLabels) / len(testLabels)\n",
        "    #accuracy.append(acc)\n",
        "\n",
        "\n",
        "    #print(loss,acc)\n",
        "    print(\"Ending Test Accuracy: \",   acc)\n",
        "    print(\"Ending Test Loss: \",logloss)\n",
        "\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(range(epochs), loss, label=\"Log Loss\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Log Loss\")\n",
        "    plt.title(\"Log Loss over Epochs\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    # Plot accuracy over epochs\n",
        "    plt.figure()\n",
        "    plt.plot(range(epochs), accuracy, label=\"Accuracy\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.title(\"Accuracy over Epochs\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "base()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inlck7g6Jp60"
      },
      "source": [
        "#CNN WITH MULTIPLE KERNELS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0F8k6aKiv_G"
      },
      "source": [
        "##Framework"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yWFuhhw7JtHP"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "from scipy import signal\n",
        "from abc import ABC, abstractmethod\n",
        "import random\n",
        "import sys\n",
        "import os\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "\n",
        "########## BASE CLASS ###########\n",
        "class Layer(ABC):\n",
        "    def __init__(self):\n",
        "        self.__prevIn = []\n",
        "        self.__prevOut = []\n",
        "\n",
        "    def setPrevIn(self, dataIn):\n",
        "        self.__prevIn = dataIn\n",
        "\n",
        "    def setPrevOut(self, out):\n",
        "        self.__prevOut = out\n",
        "\n",
        "    def getPrevIn(self):\n",
        "        return self.__prevIn\n",
        "\n",
        "    def getPrevOut(self):\n",
        "        return self.__prevOut\n",
        "\n",
        "    def backward(self, gradIn):\n",
        "        sg = self.gradient()\n",
        "        if sg.ndim == 3:\n",
        "            gradOut = np.zeros((gradIn.shape[0], sg.shape[2]))\n",
        "            for i in range(gradIn.shape[0]):\n",
        "                gradOut[i] = np.atleast_2d(gradIn[i]) @ np.atleast_2d(sg[i])\n",
        "        else:\n",
        "            gradOut = np.atleast_2d(gradIn) @ sg\n",
        "        return gradOut\n",
        "\n",
        "    @abstractmethod\n",
        "    def forward(self, dataIn):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def gradient(self):\n",
        "        pass\n",
        "\n",
        "\n",
        "########## CONVOLUTIONAL LAYER WITH MULTIPLE KERNELS ###########\n",
        "class ConvolutionalLayer(Layer):\n",
        "    def __init__(self, kernel_size, num_kernels):\n",
        "        super().__init__()\n",
        "        self.kernel_size = kernel_size\n",
        "        self.num_kernels = num_kernels\n",
        "        self.kernels = np.random.randn(num_kernels, kernel_size, kernel_size) * 0.1  # Small random values\n",
        "\n",
        "    def setKernels(self, K):\n",
        "        self.kernels = K\n",
        "\n",
        "    def getKernels(self):\n",
        "        return self.kernels\n",
        "\n",
        "    @staticmethod\n",
        "    def crossCorrelate2D(K, X):\n",
        "        \"\"\"Computes the cross-correlation of kernel K with input X.\"\"\"\n",
        "        m = K.shape[0]\n",
        "        H, W = X.shape\n",
        "        outH, outW = H - m + 1, W - m + 1\n",
        "        result = np.zeros((outH, outW), dtype=float)\n",
        "\n",
        "        for i in range(outH):\n",
        "            for j in range(outW):\n",
        "                result[i, j] = np.sum(X[i : i + m, j : j + m] * K)\n",
        "\n",
        "        return result\n",
        "\n",
        "    def forward(self, dataIn):\n",
        "        \"\"\"Applies multiple kernels to the input.\"\"\"\n",
        "        self.setPrevIn(dataIn)\n",
        "        N, H, W = dataIn.shape\n",
        "        m = self.kernel_size\n",
        "        outH, outW = H - m + 1, W - m + 1\n",
        "\n",
        "        output = np.zeros((N, self.num_kernels, outH, outW), dtype=float)\n",
        "        for n in range(N):\n",
        "            for k in range(self.num_kernels):\n",
        "                output[n, k] = ConvolutionalLayer.crossCorrelate2D(self.kernels[k], dataIn[n])\n",
        "\n",
        "        self.setPrevOut(output)\n",
        "        return output\n",
        "\n",
        "    def updateKernels(self, gradIn, eta):\n",
        "        \"\"\"Updates each kernel using the gradient.\"\"\"\n",
        "        N, K, outH, outW = gradIn.shape\n",
        "        dK = np.zeros_like(self.kernels)\n",
        "\n",
        "        for n in range(N):\n",
        "            for k in range(self.num_kernels):\n",
        "                for i in range(outH):\n",
        "                    for j in range(outW):\n",
        "                        patch = self.getPrevIn()[n, i : i + self.kernel_size, j : j + self.kernel_size]\n",
        "                        dK[k] += patch * gradIn[n, k, i, j]\n",
        "\n",
        "        self.kernels -= eta * dK\n",
        "\n",
        "    def gradient(self):\n",
        "      return np.ones_like(self.kernels)  # Placeholder; implement real computation\n",
        "\n",
        "\n",
        "\n",
        "########## FLATTENING LAYER ###########\n",
        "class FlatteningLayer(Layer):\n",
        "    def forward(self, dataIn):\n",
        "        self.setPrevIn(dataIn)\n",
        "        N = dataIn.shape[0]\n",
        "        out = dataIn.reshape(N, -1)  # Flattening across all dimensions\n",
        "        self.setPrevOut(out)\n",
        "        return out\n",
        "\n",
        "    def backward(self, gradIn):\n",
        "        return gradIn.reshape(self.getPrevIn().shape)\n",
        "\n",
        "    def backward2(self, gradIn):\n",
        "        return gradIn.reshape(self.getPrevIn().shape)\n",
        "\n",
        "    def gradient(self):\n",
        "      return np.ones_like(self.getPrevOut())  # Ensure it's not None\n",
        "\n",
        "\n",
        "\n",
        "########## FULLY CONNECTED LAYER ###########\n",
        "class FullyConnectedLayer(Layer):\n",
        "    def __init__(self, sizeIn, sizeOut):\n",
        "        super().__init__()\n",
        "        #trying Xavier initialization to see if it leads anywhere\n",
        "        #self.W = np.random.uniform(-1e-4, 1e-4, (sizeIn, sizeOut))\n",
        "        #self.b = np.random.uniform(-1e-4, 1e-4, (1, sizeOut))\n",
        "\n",
        "        #Xaiver\n",
        "        a =  -(6/(sizeIn+sizeOut))**(1/2)\n",
        "        b = (6/(sizeIn+sizeOut))**(1/2)\n",
        "        self.W = np.random.uniform(a, b, (sizeIn, sizeOut))\n",
        "        self.B = np.random.uniform(a, b, (1, sizeOut))\n",
        "\n",
        "    def getWeights(self):\n",
        "        return self.W\n",
        "\n",
        "    def setWeights(self, W):\n",
        "        self.W = W\n",
        "\n",
        "    def getBiases(self):\n",
        "        return self.B\n",
        "\n",
        "    def setBiases(self, b):\n",
        "        self.B = b\n",
        "\n",
        "    def forward(self, dataIn):\n",
        "\n",
        "        self.setPrevIn(dataIn)\n",
        "        result = np.dot(dataIn, self.W) + self.B\n",
        "        self.setPrevOut(result)\n",
        "        return result\n",
        "\n",
        "    def gradient(self):\n",
        "        return np.tile(self.W.T, (self.getPrevIn().shape[0], 1, 1))\n",
        "\n",
        "    def backward(self, gradIn):\n",
        "        gradOut = super().backward(gradIn)\n",
        "        dW = np.dot(self.getPrevIn().T, gradIn)\n",
        "        db = np.sum(gradIn, axis=0, keepdims=True)\n",
        "        self.dW = dW\n",
        "        self.db = db\n",
        "        return gradOut\n",
        "\n",
        "    def backward2(self, gradIn):\n",
        "        return np.dot(gradIn, self.W.T)\n",
        "\n",
        "    def updateWeights(self,gradIn, eta):\n",
        "\n",
        "        dJdb = np.sum(gradIn, axis = 0)/gradIn.shape[0]\n",
        "        dJdW = (self.getPrevIn().T @ gradIn)/gradIn.shape[0]\n",
        "\n",
        "        self.W -= eta*dJdW\n",
        "        self.B -= eta*dJdb\n",
        "\n",
        "\n",
        "########## MAX POOLING LAYER ###########\n",
        "class MaxPoolLayer(Layer):\n",
        "    def __init__(self, pool_size, stride):\n",
        "        super().__init__()\n",
        "        self.pool_size = pool_size\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, dataIn):\n",
        "        self.setPrevIn(dataIn)\n",
        "        N, K, H, W = dataIn.shape  # dataIn has shape (N, K, H, W)\n",
        "        ps, st = self.pool_size, self.stride\n",
        "        outH, outW = (H - ps) // st + 1, (W - ps) // st + 1\n",
        "\n",
        "        output = np.zeros((N, K, outH, outW), dtype=float)\n",
        "        self.max_indices = np.zeros((N, K, outH, outW, 2), dtype=int)\n",
        "\n",
        "        # Apply max pooling for each kernel (feature map)\n",
        "        for n in range(N):\n",
        "            for k in range(K):  # Loop over each channel (kernel's output feature map)\n",
        "                for i in range(outH):\n",
        "                    for j in range(outW):\n",
        "                        r_start, c_start = i * st, j * st\n",
        "                        region = dataIn[n, k, r_start:r_start+ps, c_start:c_start+ps]\n",
        "                        output[n, k, i, j] = np.max(region)\n",
        "                        idx = np.unravel_index(np.argmax(region), region.shape)\n",
        "                        self.max_indices[n, k, i, j] = idx\n",
        "\n",
        "        self.setPrevOut(output)\n",
        "        return output\n",
        "\n",
        "    def gradient(self):\n",
        "        pass\n",
        "\n",
        "    def backward(self, gradIn):\n",
        "        N, K, H, W = self.getPrevIn().shape  # dataIn has shape (N, K, H, W)\n",
        "\n",
        "        # Initialize gradient output (same shape as the input to the max pooling layer)\n",
        "        gradOut = np.zeros((N, K, H, W), dtype=float)\n",
        "\n",
        "        # Loop through the batch and channels (feature maps from different kernels)\n",
        "        for n in range(N):  # Loop over the batch\n",
        "            for k in range(K):  # Loop over the channels (feature maps from different kernels)\n",
        "                for i in range(gradIn.shape[2]):  # Loop over the height of the output (gradIn)\n",
        "                    for j in range(gradIn.shape[3]):  # Loop over the width of the output (gradIn)\n",
        "                        # Get the index of the maximum value from the forward pass\n",
        "                        max_idx = self.max_indices[n, k, i, j]\n",
        "\n",
        "                        # Convert the flattened index back to 2D (row, column)\n",
        "                        max_i, max_j = np.unravel_index(max_idx, (self.pool_size, self.pool_size))\n",
        "\n",
        "                        # Calculate the start position of the pooling region\n",
        "                        start_i = i * self.stride\n",
        "                        start_j = j * self.stride\n",
        "\n",
        "                        # Calculate the global position (row, column) in the original input image\n",
        "                        global_i = start_i + max_i\n",
        "                        global_j = start_j + max_j\n",
        "\n",
        "                        # Update the gradient at the global position for the current feature map\n",
        "                        gradOut[n, k, global_i, global_j] += gradIn[n, k, i, j]\n",
        "\n",
        "        return gradOut\n",
        "\n",
        "\n",
        "    def backward2(self, gradIn):\n",
        "        N, K, H, W = self.getPrevIn().shape  # dataIn has shape (N, K, H, W)\n",
        "\n",
        "        # Initialize gradient output (same shape as the input to the max pooling layer)\n",
        "        gradOut = np.zeros((N, K, H, W), dtype=float)\n",
        "\n",
        "        # Loop through the batch and channels (feature maps from different kernels)\n",
        "        for n in range(N):  # Loop over the batch\n",
        "            for k in range(K):  # Loop over the channels (feature maps from different kernels)\n",
        "                for i in range(gradIn.shape[2]):  # Loop over the height of the output (gradIn)\n",
        "                    for j in range(gradIn.shape[3]):  # Loop over the width of the output (gradIn)\n",
        "                        # Get the index of the maximum value from the forward pass\n",
        "                        max_idx = self.max_indices[n, k, i, j]\n",
        "\n",
        "                        # Convert the flattened index back to 2D (row, column)\n",
        "                        max_i, max_j = np.unravel_index(max_idx, (self.pool_size, self.pool_size))\n",
        "\n",
        "                        # Calculate the start position of the pooling region\n",
        "                        start_i = i * self.stride\n",
        "                        start_j = j * self.stride\n",
        "\n",
        "                        # Calculate the global position (row, column) in the original input image\n",
        "                        global_i = start_i + max_i\n",
        "                        global_j = start_j + max_j\n",
        "\n",
        "                        # Update the gradient at the global position for the current feature map\n",
        "                        gradOut[n, k, global_i, global_j] += gradIn[n, k, i, j]\n",
        "\n",
        "        return gradOut\n",
        "\n",
        "\n",
        "\n",
        "########## LOG LOSS ###########\n",
        "class LogLoss:\n",
        "    def eval(self, Y, Yhat):\n",
        "        eps = 1e-7\n",
        "        Yhat = np.clip(Yhat, eps, 1 - eps)\n",
        "        return -np.mean(Y * np.log(Yhat) + (1 - Y) * np.log(1 - Yhat))\n",
        "\n",
        "    def gradient(self, Y, Yhat):\n",
        "        eps = 1e-7\n",
        "        Yhat = np.clip(Yhat, eps, 1 - eps)\n",
        "        return (1 - Y) / (1 - Yhat) - Y / Yhat\n",
        "\n",
        "\n",
        "########## LOGISTIC SIGMOID LAYER ###########\n",
        "class LogisticSigmoidLayer(Layer):\n",
        "    def forward(self, dataIn):\n",
        "        self.setPrevIn(dataIn)\n",
        "        dataIn = np.clip(dataIn, -500, 500)\n",
        "        result = 1 / (1 + np.exp(-dataIn))\n",
        "        self.setPrevOut(result)\n",
        "        return result\n",
        "\n",
        "    def backward(self, gradIn):\n",
        "        sigmoid = self.getPrevOut()\n",
        "        return gradIn * (sigmoid * (1 - sigmoid))\n",
        "\n",
        "    def gradient(self):\n",
        "        pass\n",
        "\n",
        "    def gradient2(self):\n",
        "        G = self.getPrevOut() * (1 - self.getPrevOut())\n",
        "        return G\n",
        "\n",
        "    def backward2(self,gradIn):\n",
        "        gradOut = gradIn*self.gradient2()\n",
        "        return gradOut\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSSYNerci0HY"
      },
      "source": [
        "##Run Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "p7OwxPzxJw8A",
        "outputId": "a6f0fa0a-e78a-4546-ea1b-06998a9d0936"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Begin Training\n",
            "Epoch 10: Accuracy 0.5450, Loss 0.6865\n",
            "Epoch 20: Accuracy 0.5450, Loss 0.6847\n",
            "Epoch 30: Accuracy 0.5550, Loss 0.6841\n",
            "Epoch 40: Accuracy 0.5600, Loss 0.6838\n",
            "Epoch 50: Accuracy 0.5600, Loss 0.6835\n",
            "Epoch 60: Accuracy 0.5600, Loss 0.6831\n",
            "Epoch 70: Accuracy 0.5600, Loss 0.6827\n",
            "Epoch 80: Accuracy 0.5600, Loss 0.6823\n",
            "Epoch 90: Accuracy 0.5600, Loss 0.6821\n",
            "Last 100 Epochs: 3408.81 seconds\n",
            "Epoch 100: Accuracy 0.5600, Loss 0.6819\n",
            "Epoch 110: Accuracy 0.5600, Loss 0.6818\n",
            "Epoch 120: Accuracy 0.5600, Loss 0.6816\n",
            "Epoch 130: Accuracy 0.5600, Loss 0.6814\n",
            "Epoch 140: Accuracy 0.5600, Loss 0.6813\n",
            "Epoch 150: Accuracy 0.5600, Loss 0.6813\n",
            "Epoch 160: Accuracy 0.5600, Loss 0.6811\n",
            "Epoch 170: Accuracy 0.5600, Loss 0.6809\n",
            "Epoch 180: Accuracy 0.5600, Loss 0.6809\n",
            "Epoch 190: Accuracy 0.5600, Loss 0.6807\n",
            "Last 100 Epochs: 3408.04 seconds\n",
            "Epoch 200: Accuracy 0.5600, Loss 0.6804\n",
            "Epoch 210: Accuracy 0.5600, Loss 0.6800\n",
            "Epoch 220: Accuracy 0.5600, Loss 0.6796\n",
            "Epoch 230: Accuracy 0.5600, Loss 0.6793\n",
            "Epoch 240: Accuracy 0.5600, Loss 0.6791\n",
            "Final Training Accuracy: 0.5600\n",
            "Final Training Loss: 0.6789\n",
            "Final Test Accuracy: 0.5000\n",
            "Final Test Loss: 0.7115\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import numpy as np\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def train_cnn():\n",
        "    #kernel parameters\n",
        "\n",
        "    # Load and resize images\n",
        "    testSet, trainSet, testLabels, trainLabels = [], [], [], []\n",
        "\n",
        "    testFakeImg, testFakeLabels = loadImages(fpath, 'test', 0, testFalse)\n",
        "    testRealImg, testRealLabels = loadImages(fpath, 'test', 1, testTrue)\n",
        "    trainFakeImg, trainFakeLabels = loadImages(fpath, 'train', 0, trainFalse)\n",
        "    trainRealImg, trainRealLabels = loadImages(fpath, 'train', 1, trainTrue)\n",
        "\n",
        "    testSet.extend(testFakeImg + testRealImg)\n",
        "    testLabels.extend(testFakeLabels + testRealLabels)\n",
        "    trainSet.extend(trainFakeImg + trainRealImg)\n",
        "    trainLabels.extend(trainFakeLabels + trainRealLabels)\n",
        "\n",
        "    testLabels = np.array(testLabels).reshape(-1, 1)\n",
        "    trainLabels = np.array(trainLabels).reshape(-1, 1)\n",
        "\n",
        "    # Shuffle data\n",
        "    testSet, trainSet = np.array(testSet), np.array(trainSet)\n",
        "    testShuffle, trainShuffle = np.random.permutation(len(testSet)), np.random.permutation(len(trainSet))\n",
        "    testSet, testLabels = testSet[testShuffle], testLabels[testShuffle]\n",
        "    trainSet, trainLabels = trainSet[trainShuffle], trainLabels[trainShuffle]\n",
        "\n",
        "    #Calc Dimensions\n",
        "\n",
        "    # First convolution layer (kernel size: 5x5)\n",
        "    hConv = H - kSize + 1\n",
        "    wConv = W - kSize + 1\n",
        "\n",
        "    # First MaxPool layer (size: 2, stride: 2)\n",
        "    hPool = ((hConv - mpSize) // mpStride) + 1\n",
        "    wPool = ((wConv - mpSize) // mpStride) + 1\n",
        "\n",
        "    # Flattening layer\n",
        "    flatSize = hPool * wPool * numKernels\n",
        "\n",
        "\n",
        "\n",
        "    # Initialize layers\n",
        "    L1 = ConvolutionalLayer(kSize, numKernels)\n",
        "    L2 = MaxPoolLayer(mpSize, mpStride)\n",
        "    L3 = FlatteningLayer()\n",
        "    L4 = FullyConnectedLayer(flatSize, weightOut)\n",
        "    L5 = LogisticSigmoidLayer()\n",
        "    L6 = LogLoss()\n",
        "    layers = [L1, L2, L3, L4, L5, L6]\n",
        "\n",
        "    loss, accuracy = [], []\n",
        "    tic = time.perf_counter()\n",
        "\n",
        "    print(\"Begin Training\")\n",
        "    for epoch in range(epochs):\n",
        "        if epoch % 100 == 0 and epoch != 0:\n",
        "            toc = time.perf_counter()\n",
        "            print(f\"Last 100 Epochs: {toc - tic:.2f} seconds\")\n",
        "            tic = time.perf_counter()\n",
        "\n",
        "        trainShuffle = np.random.permutation(len(trainSet))\n",
        "        trainSet, trainLabels = trainSet[trainShuffle], trainLabels[trainShuffle]\n",
        "        X = trainSet.copy()\n",
        "\n",
        "        # Forward pass\n",
        "        for layer in layers[:-1]:\n",
        "            X = layer.forward(X)\n",
        "        logloss = layers[-1].eval(trainLabels, X)\n",
        "        loss.append(logloss)\n",
        "\n",
        "        acc = np.mean((X >= 0.5).astype(int) == trainLabels)\n",
        "        accuracy.append(acc)\n",
        "\n",
        "        if epoch % 10 == 0 and epoch != 0:\n",
        "            print(f\"Epoch {epoch}: Accuracy {acc:.4f}, Loss {logloss:.4f}\")\n",
        "\n",
        "        # Backward pass\n",
        "        grad = layers[-1].gradient(trainLabels, X)\n",
        "        for i in range(len(layers) - 2, 0, -1):\n",
        "            newgrad = layers[i].backward2(grad)\n",
        "            if isinstance(layers[i], FullyConnectedLayer):\n",
        "                layers[i].updateWeights(grad, FCLearn)\n",
        "            grad = newgrad\n",
        "        layers[0].updateKernels(grad, KLearn)\n",
        "\n",
        "    print(f\"Final Training Accuracy: {accuracy[-1]:.4f}\")\n",
        "    print(f\"Final Training Loss: {loss[-1]:.4f}\")\n",
        "\n",
        "    # Evaluation on test set\n",
        "    X = testSet.copy()\n",
        "    for layer in layers[:-1]:\n",
        "        X = layer.forward(X)\n",
        "    logloss = layers[-1].eval(testLabels, X)\n",
        "    test_acc = np.mean((X >= 0.5) == testLabels)\n",
        "\n",
        "    print(f\"Final Test Accuracy: {test_acc:.4f}\")\n",
        "    print(f\"Final Test Loss: {logloss:.4f}\")\n",
        "\n",
        "    # Plot loss and accuracy\n",
        "    plt.figure()\n",
        "    plt.plot(range(epochs), loss, label=\"Log Loss\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.title(\"Loss over Epochs\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(range(epochs), accuracy, label=\"Accuracy\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.title(\"Accuracy over Epochs\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train_cnn()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19Khpy4eJxox"
      },
      "source": [
        "#CNN WITH RGB CHANNELS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_24f_hdWi4Zr"
      },
      "source": [
        "##Framework"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "59G2Xm1lJ0eD"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "from scipy import signal\n",
        "from abc import ABC, abstractmethod\n",
        "import random\n",
        "import sys\n",
        "import os\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "\n",
        "########## BASE CLASS ###########\n",
        "class Layer(ABC):\n",
        "    def __init__(self):\n",
        "        self.__prevIn = []\n",
        "        self.__prevOut = []\n",
        "\n",
        "    def setPrevIn(self, dataIn):\n",
        "        self.__prevIn = dataIn\n",
        "\n",
        "    def setPrevOut(self, out):\n",
        "        self.__prevOut = out\n",
        "\n",
        "    def getPrevIn(self):\n",
        "        return self.__prevIn\n",
        "\n",
        "    def getPrevOut(self):\n",
        "        return self.__prevOut\n",
        "\n",
        "    def backward(self, gradIn):\n",
        "        sg = self.gradient()\n",
        "        if sg.ndim == 3:\n",
        "            gradOut = np.zeros((gradIn.shape[0], sg.shape[2]))\n",
        "            for i in range(gradIn.shape[0]):\n",
        "                gradOut[i] = np.atleast_2d(gradIn[i]) @ np.atleast_2d(sg[i])\n",
        "        else:\n",
        "            gradOut = np.atleast_2d(gradIn) @ sg\n",
        "        return gradOut\n",
        "\n",
        "    @abstractmethod\n",
        "    def forward(self, dataIn):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def gradient(self):\n",
        "        pass\n",
        "\n",
        "\n",
        "########## CONVOLUTIONAL LAYER WITH MULTIPLE KERNELS ###########\n",
        "class ConvolutionalLayer(Layer):\n",
        "    def __init__(self, kernel_size, num_kernels):\n",
        "        super().__init__()\n",
        "        self.kernel_size = kernel_size\n",
        "        self.num_kernels = num_kernels\n",
        "        # Kernels should have 3 channels (R, G, B)\n",
        "        self.kernels = np.random.randn(num_kernels, kernel_size, kernel_size, 3) * 0.1  # Small random values\n",
        "\n",
        "    def setKernels(self, K):\n",
        "        self.kernels = K\n",
        "\n",
        "    def getKernels(self):\n",
        "        return self.kernels\n",
        "\n",
        "    @staticmethod\n",
        "    def crossCorrelate2D(K, X):\n",
        "        \"\"\"Computes cross-correlation of 3D kernel K with 3D input X.\"\"\"\n",
        "        m = K.shape[0]  # Kernel size\n",
        "        H, W, C = X.shape  # Input dimensions (Height, Width, 3 channels)\n",
        "\n",
        "        outH, outW = H - m + 1, W - m + 1\n",
        "        result = np.zeros((outH, outW), dtype=float)\n",
        "\n",
        "        for i in range(outH):\n",
        "            for j in range(outW):\n",
        "                # Apply kernel to all 3 channels and sum the result\n",
        "                result[i, j] = np.sum(X[i : i + m, j : j + m, :] * K)\n",
        "\n",
        "        return result\n",
        "\n",
        "    def forward(self, dataIn):\n",
        "        \"\"\"Applies multiple kernels to the input RGB images.\"\"\"\n",
        "        self.setPrevIn(dataIn)\n",
        "        N, H, W, C = dataIn.shape  # Ensure input has 4D shape (batch, height, width, channels)\n",
        "        m = self.kernel_size\n",
        "        outH, outW = H - m + 1, W - m + 1\n",
        "\n",
        "        output = np.zeros((N, self.num_kernels, outH, outW), dtype=float)\n",
        "        for n in range(N):\n",
        "            for k in range(self.num_kernels):\n",
        "                output[n, k] = ConvolutionalLayer.crossCorrelate2D(self.kernels[k], dataIn[n])\n",
        "\n",
        "        self.setPrevOut(output)\n",
        "        return output\n",
        "\n",
        "    def updateKernels(self, gradIn, eta):\n",
        "        \"\"\"Updates each kernel using the gradient.\"\"\"\n",
        "        N, K, outH, outW = gradIn.shape\n",
        "        dK = np.zeros_like(self.kernels)\n",
        "\n",
        "        for n in range(N):\n",
        "            for k in range(self.num_kernels):\n",
        "                for i in range(outH):\n",
        "                    for j in range(outW):\n",
        "                        patch = self.getPrevIn()[n, i : i + self.kernel_size, j : j + self.kernel_size]\n",
        "                        dK[k] += patch * gradIn[n, k, i, j]\n",
        "\n",
        "        self.kernels -= eta * dK\n",
        "\n",
        "    def gradient(self):\n",
        "      return np.ones_like(self.kernels)  # Placeholder; implement real computation\n",
        "\n",
        "\n",
        "\n",
        "########## FLATTENING LAYER ###########\n",
        "class FlatteningLayer(Layer):\n",
        "    def forward(self, dataIn):\n",
        "        self.setPrevIn(dataIn)\n",
        "        N = dataIn.shape[0]\n",
        "        out = dataIn.reshape(N, -1)  # Flattening across all dimensions\n",
        "        self.setPrevOut(out)\n",
        "        return out\n",
        "\n",
        "    def backward(self, gradIn):\n",
        "        return gradIn.reshape(self.getPrevIn().shape)\n",
        "\n",
        "    def gradient(self):\n",
        "      return np.ones_like(self.getPrevOut())  # Ensure it's not None\n",
        "\n",
        "    def backward2(self, gradIn):\n",
        "        return gradIn.reshape(self.getPrevIn().shape)\n",
        "\n",
        "\n",
        "\n",
        "########## FULLY CONNECTED LAYER ###########\n",
        "class FullyConnectedLayer(Layer):\n",
        "    def __init__(self, sizeIn, sizeOut):\n",
        "        super().__init__()\n",
        "        #trying Xavier initialization to see if it leads anywhere\n",
        "        #self.W = np.random.uniform(-1e-4, 1e-4, (sizeIn, sizeOut))\n",
        "        #self.b = np.random.uniform(-1e-4, 1e-4, (1, sizeOut))\n",
        "\n",
        "        #Xaiver\n",
        "        a =  -(6/(sizeIn+sizeOut))**(1/2)\n",
        "        b = (6/(sizeIn+sizeOut))**(1/2)\n",
        "        self.W = np.random.uniform(a, b, (sizeIn, sizeOut))\n",
        "        self.B = np.random.uniform(a, b, (1, sizeOut))\n",
        "    def getWeights(self):\n",
        "        return self.W\n",
        "\n",
        "    def setWeights(self, W):\n",
        "        self.W = W\n",
        "\n",
        "    def getBiases(self):\n",
        "        return self.B\n",
        "\n",
        "    def setBiases(self, b):\n",
        "        self.B = b\n",
        "\n",
        "    def forward(self, dataIn):\n",
        "\n",
        "        self.setPrevIn(dataIn)\n",
        "        result = np.dot(dataIn, self.W) + self.B\n",
        "        self.setPrevOut(result)\n",
        "        return result\n",
        "\n",
        "    def gradient(self):\n",
        "        return np.tile(self.W.T, (self.getPrevIn().shape[0], 1, 1))\n",
        "\n",
        "    def backward(self, gradIn):\n",
        "        gradOut = super().backward(gradIn)\n",
        "        dW = np.dot(self.getPrevIn().T, gradIn)\n",
        "        db = np.sum(gradIn, axis=0, keepdims=True)\n",
        "        self.dW = dW\n",
        "        self.db = db\n",
        "        return gradOut\n",
        "\n",
        "    def backward2(self, gradIn):\n",
        "        return np.dot(gradIn, self.W.T)\n",
        "\n",
        "    def updateWeights(self,gradIn, eta):\n",
        "\n",
        "        dJdb = np.sum(gradIn, axis = 0)/gradIn.shape[0]\n",
        "        dJdW = (self.getPrevIn().T @ gradIn)/gradIn.shape[0]\n",
        "\n",
        "        self.W -= eta*dJdW\n",
        "        self.B -= eta*dJdb\n",
        "\n",
        "\n",
        "########## MAX POOLING LAYER ###########\n",
        "class MaxPoolLayer(Layer):\n",
        "    def __init__(self, pool_size, stride):\n",
        "        super().__init__()\n",
        "        self.pool_size = pool_size\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, dataIn):\n",
        "        self.setPrevIn(dataIn)\n",
        "        N, K, H, W = dataIn.shape\n",
        "        ps, st = self.pool_size, self.stride\n",
        "        outH, outW = (H - ps) // st + 1, (W - ps) // st + 1\n",
        "\n",
        "        output = np.zeros((N, K, outH, outW), dtype=float)\n",
        "        self.max_indices = np.zeros((N, K, outH, outW, 2), dtype=int)\n",
        "\n",
        "        for n in range(N):\n",
        "            for k in range(K):\n",
        "                for i in range(outH):\n",
        "                    for j in range(outW):\n",
        "                        r_start, c_start = i * st, j * st\n",
        "                        region = dataIn[n, k, r_start:r_start+ps, c_start:c_start+ps]\n",
        "                        output[n, k, i, j] = np.max(region)\n",
        "                        idx = np.unravel_index(np.argmax(region), region.shape)\n",
        "                        self.max_indices[n, k, i, j] = idx\n",
        "\n",
        "        self.setPrevOut(output)\n",
        "        return output\n",
        "\n",
        "    def gradient(self):\n",
        "        pass\n",
        "\n",
        "    def backward(self, gradIn):\n",
        "      N, K, H, W = self.getPrevIn().shape  # Account for RGB (K channels)\n",
        "\n",
        "      # Initialize gradient output (same shape as the input to the max pooling layer)\n",
        "      gradOut = np.zeros((N, K, H, W), dtype=float)\n",
        "\n",
        "      for n in range(N):  # Loop over the batch\n",
        "          for k in range(K):  # Loop over the channels (R, G, B)\n",
        "              for i in range(gradIn.shape[1]):  # Loop over the height of the output (gradIn)\n",
        "                  for j in range(gradIn.shape[2]):  # Loop over the width of the output (gradIn)\n",
        "                      # Get the index of the maximum value from the forward pass\n",
        "                      max_idx = self.max_indices[n, k, i, j]\n",
        "\n",
        "                      # Convert the flattened index back to 2D (row, column)\n",
        "                      max_i, max_j = np.unravel_index(max_idx, (self.pool_size, self.pool_size))\n",
        "\n",
        "                      # Calculate the start position of the pooling region\n",
        "                      start_i = i * self.stride\n",
        "                      start_j = j * self.stride\n",
        "\n",
        "                      # Calculate the global position (row, column) in the original input image\n",
        "                      global_i = start_i + max_i\n",
        "                      global_j = start_j + max_j\n",
        "\n",
        "                      # Update the gradient at the global position for the current channel\n",
        "                      gradOut[n, k, global_i, global_j] += gradIn[n, k, i, j]\n",
        "\n",
        "      return gradOut\n",
        "\n",
        "\n",
        "    def backward2(self, gradIn):\n",
        "      N, K, H, W = self.getPrevIn().shape  # Account for RGB (K channels)\n",
        "\n",
        "      # Initialize gradient output (same shape as the input to the max pooling layer)\n",
        "      gradOut = np.zeros((N, K, H, W), dtype=float)\n",
        "\n",
        "      for n in range(N):  # Loop over the batch\n",
        "          for k in range(K):  # Loop over the channels (R, G, B)\n",
        "              for i in range(gradIn.shape[1]):  # Loop over the height of the output (gradIn)\n",
        "                  for j in range(gradIn.shape[2]):  # Loop over the width of the output (gradIn)\n",
        "                      # Get the index of the maximum value from the forward pass\n",
        "                      max_idx = self.max_indices[n, k, i, j]\n",
        "\n",
        "                      # Convert the flattened index back to 2D (row, column)\n",
        "                      max_i, max_j = np.unravel_index(max_idx, (self.pool_size, self.pool_size))\n",
        "\n",
        "                      # Calculate the start position of the pooling region\n",
        "                      start_i = i * self.stride\n",
        "                      start_j = j * self.stride\n",
        "\n",
        "                      # Calculate the global position (row, column) in the original input image\n",
        "                      global_i = start_i + max_i\n",
        "                      global_j = start_j + max_j\n",
        "\n",
        "                      # Update the gradient at the global position for the current channel\n",
        "                      gradOut[n, k, global_i, global_j] += gradIn[n, k, i, j]\n",
        "\n",
        "      return gradOut\n",
        "\n",
        "\n",
        "\n",
        "########## LOG LOSS ###########\n",
        "class LogLoss:\n",
        "    def eval(self, Y, Yhat):\n",
        "        eps = 1e-7\n",
        "        Yhat = np.clip(Yhat, eps, 1 - eps)\n",
        "        return -np.mean(Y * np.log(Yhat) + (1 - Y) * np.log(1 - Yhat))\n",
        "\n",
        "    def gradient(self, Y, Yhat):\n",
        "        eps = 1e-7\n",
        "        Yhat = np.clip(Yhat, eps, 1 - eps)\n",
        "        return (1 - Y) / (1 - Yhat) - Y / Yhat\n",
        "\n",
        "\n",
        "########## LOGISTIC SIGMOID LAYER ###########\n",
        "class LogisticSigmoidLayer(Layer):\n",
        "    def forward(self, dataIn):\n",
        "        self.setPrevIn(dataIn)\n",
        "        result = 1 / (1 + np.exp(-np.clip(dataIn, -500, 500))) #trying to avoid overflow, can toggle the clip\n",
        "        self.setPrevOut(result)\n",
        "        return result\n",
        "\n",
        "    def backward(self, gradIn):\n",
        "        sigmoid = self.getPrevOut()\n",
        "        return gradIn * (sigmoid * (1 - sigmoid))\n",
        "\n",
        "    def gradient(self):\n",
        "      pass\n",
        "\n",
        "    def gradient2(self):\n",
        "        G = self.getPrevOut() * (1 - self.getPrevOut())\n",
        "        return G\n",
        "\n",
        "    def backward2(self,gradIn):\n",
        "        gradOut = gradIn*self.gradient2()\n",
        "        return gradOut\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djrXMwfHi6wF"
      },
      "source": [
        "##Load Color Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "vRJTdGlxJ359"
      },
      "outputs": [],
      "source": [
        "def loadImages(fpath, folder, label, max_images):\n",
        "    \"\"\"\n",
        "    Loads RGB images from a given directory and assigns them a label.\n",
        "    \"\"\"\n",
        "    images = []\n",
        "    labels = []\n",
        "\n",
        "    folder_path = os.path.join(fpath, folder)\n",
        "    for subfolder in ['Fake', 'Real']:\n",
        "        spath = os.path.join(folder_path, subfolder)\n",
        "        for file in os.listdir(spath):\n",
        "            img_path = os.path.join(spath, file)\n",
        "            image = Image.open(img_path).convert(\"RGB\")\n",
        "            image = image.resize((H, W))\n",
        "            image_array = np.array(image, dtype=np.float32) / 255.0\n",
        "\n",
        "            images.append(image_array)\n",
        "            labels.append(label)\n",
        "\n",
        "            # Limit the dataset size\n",
        "            if len(images) >= max_images:\n",
        "                return images, labels\n",
        "\n",
        "    return images, labels\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwwmRHZ-i83e"
      },
      "source": [
        "##Run Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "JebjQbZ8J5xr",
        "outputId": "d8c0c5da-ef56-4f72-b3f6-914f135710d9"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Begin Training\n",
            "Epoch 10: Accuracy 0.5550, Loss 0.6893\n",
            "Epoch 20: Accuracy 0.5450, Loss 0.6893\n",
            "Epoch 30: Accuracy 0.5450, Loss 0.6895\n",
            "Epoch 40: Accuracy 0.5450, Loss 0.6900\n",
            "Epoch 50: Accuracy 0.5450, Loss 0.6913\n",
            "Epoch 60: Accuracy 0.5400, Loss 0.6934\n",
            "Epoch 70: Accuracy 0.5400, Loss 0.6941\n",
            "Epoch 80: Accuracy 0.5200, Loss 0.6953\n",
            "Epoch 90: Accuracy 0.5400, Loss 0.6911\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-833cd7b70f25>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0mtrain_cnn_rgb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-9-833cd7b70f25>\u001b[0m in \u001b[0;36mtrain_cnn_rgb\u001b[0;34m()\u001b[0m\n\u001b[1;32m     90\u001b[0m                 \u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdateWeights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFCLearn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnewgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdateKernels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKLearn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Final Training Accuracy: {accuracy[-1]:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-4be7d829f243>\u001b[0m in \u001b[0;36mupdateKernels\u001b[0;34m(self, gradIn, eta)\u001b[0m\n\u001b[1;32m    103\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m                         \u001b[0mpatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetPrevIn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mj\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m                         \u001b[0mdK\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mpatch\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgradIn\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernels\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0meta\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "def train_cnn_rgb():\n",
        "    # Load and preprocess images\n",
        "    testSet = []\n",
        "    trainSet = []\n",
        "    testLabels = []\n",
        "    trainLabels = []\n",
        "\n",
        "    testFakeImg, testFakeLabels = loadImages(fpath, 'test', 0, testFalse)\n",
        "    testRealImg, testRealLabels = loadImages(fpath, 'test', 1, testTrue)\n",
        "\n",
        "    trainFakeImg, trainFakeLabels = loadImages(fpath, 'train', 0, trainFalse)\n",
        "    trainRealImg, trainRealLabels = loadImages(fpath, 'train', 1, trainTrue)\n",
        "\n",
        "    testSet.extend(testFakeImg)\n",
        "    testSet.extend(testRealImg)\n",
        "    testLabels.extend(testFakeLabels)\n",
        "    testLabels.extend(testRealLabels)\n",
        "\n",
        "    trainSet.extend(trainFakeImg)\n",
        "    trainSet.extend(trainRealImg)\n",
        "    trainLabels.extend(trainFakeLabels)\n",
        "    trainLabels.extend(trainRealLabels)\n",
        "\n",
        "    testLabels = np.array(testLabels).reshape(-1,1)\n",
        "    trainLabels = np.array(trainLabels).reshape(-1,1)\n",
        "\n",
        "    # Convert to NumPy arrays\n",
        "    testSet = np.array(testSet)  # Should be (N, H, W, 3)\n",
        "    trainSet = np.array(trainSet)\n",
        "\n",
        "    # Shuffle training data\n",
        "    trainIndicies = np.arange(trainSet.shape[0])\n",
        "    np.random.shuffle(trainIndicies)\n",
        "    trainSet = trainSet[trainIndicies]\n",
        "    trainLabels = trainLabels[trainIndicies]\n",
        "\n",
        "\n",
        "    # First convolution layer (kernel size: 5x5)\n",
        "    hConv = H - kSize + 1\n",
        "    wConv = W - kSize + 1\n",
        "\n",
        "    # First MaxPool layer (size: 2, stride: 2)\n",
        "    hPool = ((hConv - mpSize) // mpStride) + 1\n",
        "    wPool = ((wConv - mpSize) // mpStride) + 1\n",
        "\n",
        "    # Flattening layer\n",
        "    flatSize = hPool * wPool * 3\n",
        "\n",
        "    # Initialize layers\n",
        "    L1 = ConvolutionalLayer(kSize, 3)\n",
        "    L2 = MaxPoolLayer(mpSize, mpStride)\n",
        "    L3 = FlatteningLayer()\n",
        "    L4 = FullyConnectedLayer(flatSize, weightOut)\n",
        "    L5 = LogisticSigmoidLayer()\n",
        "    L6 = LogLoss()\n",
        "\n",
        "    layers = [L1, L2, L3, L4, L5, L6]\n",
        "\n",
        "    loss, accuracy = [], []\n",
        "    tic = time.perf_counter()\n",
        "\n",
        "    print(\"Begin Training\")\n",
        "    for epoch in range(100):\n",
        "        if epoch % 100 == 0 and epoch != 0:\n",
        "            toc = time.perf_counter()\n",
        "            print(f\"Last 100 Epochs: {toc - tic:.2f} seconds\")\n",
        "            tic = time.perf_counter()\n",
        "\n",
        "        trainShuffle = np.random.permutation(len(trainSet))\n",
        "        trainSet, trainLabels = trainSet[trainShuffle], trainLabels[trainShuffle]\n",
        "        X = trainSet.copy()\n",
        "\n",
        "        # Forward pass\n",
        "        for layer in layers[:-1]:\n",
        "            X = layer.forward(X)\n",
        "        logloss = layers[-1].eval(trainLabels, X)\n",
        "        loss.append(logloss)\n",
        "\n",
        "        acc = np.mean((X >= 0.5).astype(int) == trainLabels)\n",
        "        accuracy.append(acc)\n",
        "\n",
        "        if epoch % 10 == 0 and epoch != 0:\n",
        "            print(f\"Epoch {epoch}: Accuracy {acc:.4f}, Loss {logloss:.4f}\")\n",
        "\n",
        "        # Backward pass\n",
        "        grad = layers[-1].gradient(trainLabels, X)\n",
        "        for i in range(len(layers) - 2, 0, -1):\n",
        "            newgrad = layers[i].backward2(grad)\n",
        "            if isinstance(layers[i], FullyConnectedLayer):\n",
        "                layers[i].updateWeights(grad, FCLearn)\n",
        "            grad = newgrad\n",
        "        layers[0].updateKernels(grad, KLearn)\n",
        "\n",
        "    print(f\"Final Training Accuracy: {accuracy[-1]:.4f}\")\n",
        "    print(f\"Final Training Loss: {loss[-1]:.4f}\")\n",
        "\n",
        "    # Evaluation on test set\n",
        "    X = testSet.copy()\n",
        "    for layer in layers[:-1]:\n",
        "        X = layer.forward(X)\n",
        "    logloss = layers[-1].eval(testLabels, X)\n",
        "    test_acc = np.mean((X >= 0.5) == testLabels)\n",
        "\n",
        "    print(f\"Final Test Accuracy: {test_acc:.4f}\")\n",
        "    print(f\"Final Test Loss: {logloss:.4f}\")\n",
        "\n",
        "    # Plot loss and accuracy\n",
        "    plt.figure()\n",
        "    plt.plot(range(epochs), loss, label=\"Log Loss\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.title(\"Loss over Epochs\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(range(epochs), accuracy, label=\"Accuracy\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.title(\"Accuracy over Epochs\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train_cnn_rgb()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EeRIcOZz31dO"
      },
      "source": [
        "#CNN WITH MULTIPLE CONVOLUTION LAYERS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzptlFt5hqf1"
      },
      "source": [
        "##Framework\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gu-Y9IwD37EQ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "from scipy import signal\n",
        "from abc import ABC, abstractmethod\n",
        "import random\n",
        "import sys\n",
        "import os\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "\n",
        "##########BASE CLASS###########\n",
        "class Layer(ABC):\n",
        "  def __init__(self):\n",
        "    self.__prevIn = []\n",
        "    self.__prevOut = []\n",
        "\n",
        "  def setPrevIn(self,dataIn):\n",
        "    self.__prevIn = dataIn\n",
        "\n",
        "  def setPrevOut(self, out):\n",
        "    self.__prevOut = out\n",
        "\n",
        "  def getPrevIn(self):\n",
        "    return self.__prevIn\n",
        "\n",
        "  def getPrevOut(self):\n",
        "    return self.__prevOut\n",
        "\n",
        "  \"\"\"\n",
        "  def backward(self, gradIn):\n",
        "    sg = self.gradient()\n",
        "    gradOut = np.zeros((gradIn.shape[0],sg.shape[2]))\n",
        "\n",
        "    for n in range(gradIn.shape[0]):\n",
        "        gradOut[n] = np.atleast_2d(gradIn[n])@sg[n]\n",
        "    return gradOut\n",
        "  \"\"\"\n",
        "  def backward(self,gradIn):\n",
        "    #from lecture slides:\n",
        "\n",
        "    #sg = self.gradient()\n",
        "    #gradOut = np.zeros((gradIn.shape[0],sg.shape[1]))\n",
        "    #for i in range(gradIn.shape[0]):\n",
        "    #  gradOut[i] = np.atleast_2d(gradIn[i])@np.atleast_2d(sg[i])\n",
        "    #  return gradOut\n",
        "\n",
        "\n",
        "    sg = self.gradient()\n",
        "\n",
        "    if(sg.ndim == 3):  #tensor coming back\n",
        "        gradOut = np.zeros((gradIn.shape[0],sg.shape[2]))\n",
        "        for i in range(gradIn.shape[0]):\n",
        "            gradOut[i] = np.atleast_2d(gradIn[i])@np.atleast_2d(sg[i])\n",
        "    else:\n",
        "        gradOut = np.atleast_2d(gradIn)@sg\n",
        "\n",
        "    return gradOut\n",
        "\n",
        "  @abstractmethod\n",
        "  def forward(self,dataIn):\n",
        "    pass\n",
        "\n",
        "  @abstractmethod\n",
        "  def gradient(self):\n",
        "    pass\n",
        "\n",
        "##########BASE CLASS###########\n",
        "class Objective(ABC):\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def eval(Y,Yhat):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def gradient(Y,Yhat):\n",
        "        pass\n",
        "\n",
        "\n",
        "#\\/\\/\\/\\/CONVOLUTION\\/\\/\\/\\/\n",
        "class ConvolutionalLayer(Layer):\n",
        "\n",
        "    '''Constructor (init)- Your constructor should take in a single\n",
        "    explicit parameter, the kernel size.\n",
        "    We will assume the kernel is square, of an odd size,\n",
        "    and that will have a single kernel '''\n",
        "\n",
        "    def __init__(self, kernelSize):\n",
        "        super().__init__()\n",
        "        #is zeros right?\n",
        "        #check for odd number size?\n",
        "        a = -1e-2\n",
        "        b = 1e-2\n",
        "        self.kernel = np.random.uniform(a, b, (kernelSize, kernelSize))\n",
        "\n",
        "\n",
        "    def setKernels(self,K):\n",
        "        '''- This method should take a matrix (or tensor) as a parameter\n",
        "        and set the weights of the kernel(s) to it. '''\n",
        "\n",
        "        self.kernel = K\n",
        "\n",
        "    def getKernels(self):\n",
        "\n",
        "        return self.kernel\n",
        "\n",
        "    @staticmethod\n",
        "    def crossCorrelate2D(K,N):\n",
        "        '''This static method should take two matrices as parameters,\n",
        "        and return their cross-correlation. You do not need to support padding.'''\n",
        "        N_height, N_width = N.shape\n",
        "        K_height, K_width = K.shape\n",
        "\n",
        "        result_height = N_height - K_height + 1\n",
        "        result_width = N_width - K_width + 1\n",
        "\n",
        "        result = np.zeros((result_height, result_width))\n",
        "\n",
        "        for i in range(result_height):\n",
        "            for j in range(result_width):\n",
        "                submatrix = N[i: i+K_height, j: j+K_width]\n",
        "                result[i, j] = np.sum(submatrix * K)\n",
        "\n",
        "\n",
        "        return result\n",
        "\n",
        "\n",
        "    def forward(self,X):\n",
        "        '''This method should take a tensor of incoming data (N × H × W),\n",
        "        set prevIn with it, compute the cross-correlation of\n",
        "        each observations with the kernel,\n",
        "        store this result with its parent class (prevOut)\n",
        "        and return it (this output should also be a tensor). '''\n",
        "        self.setPrevIn(X)\n",
        "        N,H,W = X.shape\n",
        "        outputs = []\n",
        "\n",
        "        for i in range(N):\n",
        "\n",
        "            result = ConvolutionalLayer.crossCorrelate2D(self.kernel,X[i])\n",
        "\n",
        "            outputs.append(result)\n",
        "\n",
        "        result = np.stack(outputs,axis=0)\n",
        "        self.setPrevOut(result)\n",
        "        return result\n",
        "\n",
        "\n",
        "    def gradient(self):\n",
        "        pass\n",
        "\n",
        "    def backward2(self, backGrad):\n",
        "        ''' Backward pass: Compute the gradient with respect to the inputs. '''\n",
        "\n",
        "        # Get the previous input and kernel\n",
        "        X = self.getPrevIn()  # (N, H, W)\n",
        "        kernel = self.getKernels()  # (K_h, K_w)\n",
        "        N, H, W = X.shape  # N = batch size, H = height, W = width of input\n",
        "        K_h, K_w = kernel.shape  # Kernel height and width\n",
        "\n",
        "        # Initialize the gradient for the input (gradIn) with the same shape as the input\n",
        "        gradIn = np.zeros_like(X)\n",
        "\n",
        "        # Loop through each sample in the batch\n",
        "        for n in range(N):\n",
        "            # Loop over every position in the input (sliding window for convolution)\n",
        "            for i in range(H - K_h + 1):\n",
        "                for j in range(W - K_w + 1):\n",
        "                    # For each position, get the patch from the output gradient (backGrad)\n",
        "                    grad_patch = backGrad[n, i, j]\n",
        "\n",
        "                    # Get the region of the input that corresponds to this position\n",
        "                    input_patch = X[n, i:i + K_h, j:j + K_w]\n",
        "\n",
        "                    # Now we calculate the gradient with respect to the input by reversing the convolution operation\n",
        "                    gradIn[n, i:i + K_h, j:j + K_w] += grad_patch * kernel  # element-wise multiplication\n",
        "\n",
        "        return gradIn\n",
        "\n",
        "    def updateKernels(self,backGrad,eta):\n",
        "        X = self.getPrevIn()\n",
        "        N, H, W = X.shape\n",
        "\n",
        "        kernelGrad = np.zeros(self.kernel.shape,dtype='float64')\n",
        "\n",
        "\n",
        "        for n in range(N):\n",
        "            for i in range(H - self.kernel.shape[0] + 1):\n",
        "                for j in range(W - self.kernel.shape[1] + 1):\n",
        "                    input_patch = X[n, i:i + self.kernel.shape[0], j:j + self.kernel.shape[1]]\n",
        "\n",
        "                    kernelGrad += input_patch * backGrad[n, i, j]\n",
        "\n",
        "\n",
        "        self.kernel = self.kernel -  eta * kernelGrad\n",
        "\n",
        "#\\/\\/\\/\\/Cross Entropy\\/\\/\\/\\/\n",
        "\n",
        "class CrossEntropy(Objective):\n",
        "    def eval(self,Y,Yhat):\n",
        "        constant = 10**-7\n",
        "        tmp = Y*np.log(Yhat+constant)\n",
        "        return -np.mean(np.sum(tmp,axis=1))\n",
        "\n",
        "\n",
        "\n",
        "    def gradient(self,Y,Yhat):\n",
        "        constant = 10e-7\n",
        "        return -(Y/(Yhat + constant))\n",
        "\n",
        "\n",
        "#\\/\\/\\/\\/Flatten\\/\\/\\/\\/\n",
        "class FlatteningLayer(Layer):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, X):\n",
        "        self.setPrevIn(X)\n",
        "\n",
        "        N,H,W = X.shape\n",
        "        flatten = X.reshape(N,-1,order=\"F\")\n",
        "\n",
        "        self.setPrevOut(flatten)\n",
        "        return flatten\n",
        "\n",
        "    def gradient(self):\n",
        "        pass\n",
        "\n",
        "    def backward(self,gradIn):\n",
        "        #return original dimensions\n",
        "        N,H,W = self.getPrevIn().shape\n",
        "\n",
        "        gradOut = gradIn.reshape(N,H,W, order=\"F\")\n",
        "\n",
        "        return gradOut\n",
        "\n",
        "    def backward2(self,gradIn):\n",
        "        #return original dimensions\n",
        "        #copy -- all other functions for Q2 have a backward 2 method, this makes it easier without messing with the test script\n",
        "        N,H,W = self.getPrevIn().shape\n",
        "\n",
        "        gradOut = gradIn.reshape(N,H,W, order=\"F\")\n",
        "\n",
        "        return gradOut\n",
        "\n",
        "\n",
        "#\\/\\/\\/\\/FullyConnected\\/\\/\\/\\/\n",
        "class FullyConnectedLayer(Layer):\n",
        "     #Input : sizeIn , the number of features of data coming in\n",
        "    # #Input : sizeOut , the number of features for the data coming out.\n",
        "    # #Output: None\n",
        "\n",
        "    def __init__(self, sizeIn, sizeOut,useBias=0):\n",
        "        super().__init__()\n",
        "        #useBias needed for test, unsure where it fits in this setup\n",
        "\n",
        "        #initialize weight and bias\n",
        "        #Xaiver (move to INput)\n",
        "        a =  -(6/(sizeIn+sizeOut))**(1/2)\n",
        "        b = (6/(sizeIn+sizeOut))**(1/2)\n",
        "        self.W = np.random.uniform(a, b, (sizeIn, sizeOut))\n",
        "        self.B = np.random.uniform(a, b, (1, sizeOut))\n",
        "\n",
        "     #  #Input : None\n",
        "    # #Output: The (sizeIn by sizeOut) weight matrix.\n",
        "    def getWeights(self):\n",
        "        return self.W\n",
        "\n",
        "    # #Input : The (sizeIn by sizeOut) weight matrix.\n",
        "    # #Output: None\n",
        "    def setWeights(self, weights):\n",
        "        self.W = weights\n",
        "\n",
        "\n",
        "     # #Input : None\n",
        "    # #Output: The (1 by sizeOut) bias vector\n",
        "    def getBiases(self):\n",
        "        return self.B\n",
        "\n",
        "     # #Input : The (1 by sizeOut) bias vector\n",
        "    # #Output: None\n",
        "    def setBiases(self, biases):\n",
        "        self.B = biases\n",
        "\n",
        "     # #Input : dataIn , a (1 by D) data matrix\n",
        "    # #Output: A (1 by K) data matrix\n",
        "    def forward(self, dataIn):\n",
        "\n",
        "        self.setPrevIn(dataIn)\n",
        "        Y = np.dot(dataIn, self.W) + self.B\n",
        "        self.setPrevOut(Y)\n",
        "        return Y\n",
        "\n",
        "    # #We’ ll worry about this later ...\n",
        "    def gradient( self ):\n",
        "        #deriv = wT\n",
        "        return self.W.T\n",
        "\n",
        "    def updateWeights(self,gradIn,eta):\n",
        "        #from lecture slide\n",
        "        #bias graidnet - average of sum of gradIn (avg hadmaradnd product of bias and identity)\n",
        "        #weight - input transpose @ gradIn avged over gradient shape.\n",
        "        #both updates of X = X-learningrate*gradient calc\n",
        "        getW = self.getWeights()\n",
        "        dJdb = np.sum(gradIn, axis=0)/gradIn.shape[0]\n",
        "        dJdW = (self.getPrevIn().T @ gradIn)/gradIn.shape[0]\n",
        "\n",
        "\n",
        "        self.W = self.W -  eta*dJdW\n",
        "        self.B = self.B - eta*dJdb\n",
        "\n",
        "    def backward2(self,gradIn):\n",
        "\n",
        "        gradOut = np.dot(gradIn, self.gradient())\n",
        "        return gradOut\n",
        "\n",
        "#\\/\\/\\/\\/Input\\/\\/\\/\\/\n",
        "class InputLayer(Layer):\n",
        "    #Input : dataIn , an (N by D) matrix\n",
        "    # #Output: None\n",
        "\n",
        "    def __init__( self , dataIn,z_score=0 ):\n",
        "        #take the mean and standard deviation of each column\n",
        "        if z_score != 0:\n",
        "            self.meanX = np.mean(dataIn,axis=0)\n",
        "            self.stdX = np.std(dataIn,axis=0,ddof=1)\n",
        "            self.stdX[self.stdX == 0] = 1\n",
        "        else:\n",
        "           self.meanX = 0\n",
        "           self.stdX = 1\n",
        "\n",
        "    # #Input : dataIn , a (1 by D) matrix\n",
        "    # #Output: A (1 by D) matrix\n",
        "    def forward( self , dataIn ):\n",
        "        #review unsure of the 1 x D and the stored attributes\n",
        "        self.setPrevIn(dataIn)\n",
        "        zscored = (dataIn-self.meanX)/self.stdX\n",
        "        self.setPrevOut(zscored)\n",
        "\n",
        "        return zscored\n",
        "\n",
        "    # #We’ ll worry about this later ...\n",
        "    def gradient( self ):\n",
        "     pass\n",
        "\n",
        "#\\/\\/\\/\\/Linear\\/\\/\\/\\/\n",
        "\n",
        "class LinearLayer(Layer):\n",
        "    #Input : None\n",
        "    # #Output: None\n",
        "    def __init__ ( self ):\n",
        "        super().__init__()\n",
        "\n",
        "\n",
        "    #Input : dataIn , a (1 by K) matrix\n",
        "    # #Output: A (1 by K) matrix\n",
        "    def forward( self , dataIn ):\n",
        "        self.setPrevIn(dataIn)\n",
        "        self.setPrevOut(dataIn)\n",
        "        return dataIn\n",
        "\n",
        "\n",
        "\n",
        "    # #We’ ll worry about this later ...\n",
        "    def gradient( self ):\n",
        "        #identity matrix\n",
        "        m,n = self.getPrevIn().shape\n",
        "        return np.eye(n)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#\\/\\/\\/\\/Sigmoid\\/\\/\\/\\/\n",
        "class LogisticSigmoidLayer(Layer):\n",
        "    #Input : None\n",
        "    # #Output: None\n",
        "    def __init__ ( self ):\n",
        "        super().__init__()\n",
        "\n",
        "    #Input : dataIn , a (1 by K) matrix\n",
        "    # #Output: A (1 by K) matrix\n",
        "    def forward( self , dataIn ):\n",
        "        self.setPrevIn(dataIn)\n",
        "\n",
        "        clippedData = np.clip(dataIn, -500, 500)\n",
        "\n",
        "        Y = 1/(1+np.exp(-clippedData))\n",
        "        self.setPrevOut(Y)\n",
        "        return Y\n",
        "\n",
        "\n",
        "    # #We’ ll worry about this later ...\n",
        "    def gradient( self ):\n",
        "        m, n = self.getPrevIn().shape\n",
        "\n",
        "        # Calculate sigmoid derivative for each element\n",
        "        G = self.getPrevOut() * (1 - self.getPrevOut())\n",
        "\n",
        "        # Initialize output array with proper shape (m,n,n)\n",
        "        gradient_matrices = np.zeros((m, n, n))\n",
        "\n",
        "        # For each sample, create a diagonal matrix\n",
        "        for i in range(m):\n",
        "            gradient_matrices[i] = np.diag(G[i])\n",
        "\n",
        "        return gradient_matrices\n",
        "\n",
        "    def gradient2(self):\n",
        "        G = self.getPrevOut() * (1 - self.getPrevOut())\n",
        "        return G\n",
        "\n",
        "    def backward2(self,gradIn):\n",
        "        gradOut = gradIn*self.gradient2()\n",
        "        return gradOut\n",
        "\n",
        "#\\/\\/\\/\\/LogLoss\\/\\/\\/\\/\n",
        "class LogLoss(Objective):\n",
        "    def eval(self,Y,Yhat):\n",
        "        constant = 10**-7\n",
        "        return np.mean(-((Y*np.log(Yhat+constant))+(1-Y)*np.log(1-Yhat+constant)))\n",
        "\n",
        "    def gradient(self,Y,Yhat):\n",
        "        constant = 10**-7\n",
        "        #Y = np.expand_dims(Y, axis=1) # placeholder -- broadcasting issue\n",
        "\n",
        "        return np.atleast_2d((1-Y)/(1-Yhat+constant)-Y/(Yhat+constant))\n",
        "\n",
        "#\\/\\/\\/\\/MaxPoolLayer\\/\\/\\/\\/\n",
        "class MaxPoolLayer(Layer):\n",
        "    def __init__ ( self,size,stride ):\n",
        "        super().__init__()\n",
        "        self.size = size\n",
        "        self.stride = stride\n",
        "        self.indices = None\n",
        "\n",
        "    def forward(self,X):\n",
        "        self.setPrevIn(X)\n",
        "        N, H, W = X.shape\n",
        "\n",
        "        out_height = (H - self.size) // self.stride + 1\n",
        "        out_width = (W - self.size) // self.stride + 1\n",
        "\n",
        "        output = np.zeros((N, out_height, out_width))\n",
        "        self.indices = np.zeros((N, out_height, out_width), dtype=int)  # To store indices of max values\n",
        "\n",
        "\n",
        "        for n in range(N):\n",
        "            for i in range(out_height):\n",
        "                for j in range(out_width):\n",
        "\n",
        "                    start_i = i * self.stride\n",
        "                    start_j = j * self.stride\n",
        "                    end_i = start_i + self.size\n",
        "                    end_j = start_j + self.size\n",
        "\n",
        "\n",
        "                    window = X[n, start_i:end_i, start_j:end_j]\n",
        "\n",
        "\n",
        "                    maxVal = np.max(window)\n",
        "                    maxIdx = np.argmax(window)\n",
        "\n",
        "\n",
        "                    output[n, i, j] = maxVal\n",
        "\n",
        "                    self.indices[n, i, j] = maxIdx\n",
        "\n",
        "        self.setPrevOut(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def gradient(self):\n",
        "        pass\n",
        "\n",
        "    def backward(self,gradIn):\n",
        "        N, H, W = self.getPrevIn().shape\n",
        "\n",
        "        gradOut = np.zeros((N,H,W))\n",
        "\n",
        "        for n in range(N):\n",
        "            for i in range(gradIn.shape[1]):\n",
        "                for j in range(gradIn.shape[2]):\n",
        "                    max_idx = self.indices[n, i, j]\n",
        "\n",
        "                    # Converting flattened index back\n",
        "                    max_i, max_j = np.unravel_index(max_idx, (self.size, self.size))\n",
        "\n",
        "                    start_i = i * self.stride\n",
        "                    start_j = j * self.stride\n",
        "\n",
        "                    global_i = start_i + max_i\n",
        "                    global_j = start_j + max_j\n",
        "\n",
        "                    gradOut[n, global_i, global_j] += gradIn[n, i, j]\n",
        "\n",
        "        return gradOut\n",
        "\n",
        "\n",
        "    def backward2(self,gradIn):\n",
        "        #copy -- all other functions for Q2 have a backward 2 method,\n",
        "        # this makes it easier without messing with the test script\n",
        "\n",
        "        N, H, W = self.getPrevIn().shape\n",
        "\n",
        "        gradOut = np.zeros((N,H,W))\n",
        "\n",
        "        for n in range(N):\n",
        "            for i in range(gradIn.shape[1]):\n",
        "                for j in range(gradIn.shape[2]):\n",
        "                    max_idx = self.indices[n, i, j]\n",
        "\n",
        "                    # Converting flattened index back\n",
        "                    max_i, max_j = np.unravel_index(max_idx, (self.size, self.size))\n",
        "\n",
        "                    start_i = i * self.stride\n",
        "                    start_j = j * self.stride\n",
        "\n",
        "                    global_i = start_i + max_i\n",
        "                    global_j = start_j + max_j\n",
        "\n",
        "                    gradOut[n, global_i, global_j] += gradIn[n, i, j]\n",
        "\n",
        "        return gradOut\n",
        "\n",
        "#\\/\\/\\/\\/ReLU\\/\\/\\/\\/\n",
        "class ReLULayer(Layer):\n",
        "    #Input : None\n",
        "    # #Output: None\n",
        "    def __init__ ( self ):\n",
        "        super().__init__()\n",
        "\n",
        "    #Input : dataIn , a (1 by K) matrix\n",
        "    # #Output: A (1 by K) matrix\n",
        "    def forward( self , dataIn ):\n",
        "        self.setPrevIn(dataIn)\n",
        "        Y = np.maximum(0,dataIn)\n",
        "        self.setPrevOut(Y)\n",
        "        return Y\n",
        "\n",
        "\n",
        "    # #We’ ll worry about this later ...\n",
        "    def gradient( self ):\n",
        "        #near-identity, but value set to 0 if z < 0\n",
        "        m,n = self.getPrevIn().shape\n",
        "        I = np.eye(n)\n",
        "        I[I<0] = 0\n",
        "        return I\n",
        "\n",
        "    def backward2(self, gradIn):\n",
        "        # Create a mask for the derivative of ReLU (1 for positive values, 0 for non-positive)\n",
        "        gradOut = gradIn * (self.getPrevIn() > 0).astype(float)\n",
        "        return gradOut  # Return the gradient that should propagate backward\n",
        "\n",
        "#\\/\\/\\/\\/SOFTMAX\\/\\/\\/\\/\n",
        "class SoftmaxLayer(Layer):\n",
        "    #Input : None\n",
        "    # #Output: None\n",
        "    def __init__ ( self ):\n",
        "        super().__init__()\n",
        "\n",
        "    #Input : dataIn , a (1 by K) matrix\n",
        "    # #Output: A (1 by K) matrix\n",
        "    def forward( self , dataIn ):\n",
        "        #original\n",
        "        #self.setPrevIn(dataIn)\n",
        "        #Y = (np.e**dataIn/np.sum(np.e**dataIn))\n",
        "        #self.setPrevOut(Y)\n",
        "        #return Y\n",
        "\n",
        "        #Changes for (N x K) -- compute rowwise\n",
        "        self.setPrevIn(dataIn)\n",
        "\n",
        "        Y = np.exp(dataIn)  / np.sum(np.exp(dataIn) , axis=1, keepdims=True)\n",
        "\n",
        "        self.setPrevOut(Y)\n",
        "        return Y\n",
        "\n",
        "    # #We’ ll worry about this later ...\n",
        "    def gradient( self ):\n",
        "        #on diagonals = gj(z)(1-gj(z))\n",
        "        #elsewhere = -gi(z)*gj(z)\n",
        "        #simplified = gi(z)((i==j)-gj(z))\n",
        "        #evenmore = diag(g(z))-g(z)T * g(z)\n",
        "\n",
        "        po = self.getPrevOut()\n",
        "\n",
        "\n",
        "        batch = po.shape[0]\n",
        "        K = po.shape[1]\n",
        "\n",
        "        grad = np.zeros((batch, K, K))\n",
        "\n",
        "        for b in range(batch):\n",
        "            for i in range(K):\n",
        "                for j in range(K):\n",
        "                    if i == j:\n",
        "                        grad[b, i, j] = po[b, i] * (1 - po[b, i])  # Diagonal: gi(z) * (1 - gi(z))\n",
        "                    else:\n",
        "                        grad[b, i, j] = -po[b, i] * po[b, j]  # Off-diagonal: -gi(z) * gj(z)\n",
        "\n",
        "        return grad\n",
        "\n",
        "class SquaredError(Objective):\n",
        "    def eval(self,Y,Yhat):\n",
        "        return np.mean((Y-Yhat)**2)\n",
        "\n",
        "    def gradient(self,Y,Yhat):\n",
        "        return np.atleast_2d(-2* (Y-Yhat) )\n",
        "\n",
        "    def SMAPE(self,Y,Yhat):\n",
        "        return np.mean(np.abs(Y-Yhat)/(np.abs(Y) + np.abs(Yhat)))\n",
        "\n",
        "#\\/\\/\\/\\/TANH\\/\\/\\/\\/\n",
        "class TanhLayer(Layer):\n",
        "    #Input : None\n",
        "    # #Output: None\n",
        "    def __init__ ( self ):\n",
        "        super().__init__()\n",
        "\n",
        "    #Input : dataIn , a (1 by K) matrix\n",
        "    # #Output: A (1 by K) matrix\n",
        "    def forward( self , dataIn ):\n",
        "        self.setPrevIn(dataIn)\n",
        "        Y = (np.e**dataIn - np.e**(-dataIn))/(np.e**(dataIn) + np.e**(-dataIn))\n",
        "        self.setPrevOut(Y)\n",
        "        return Y\n",
        "\n",
        "    # #We’ ll worry about this later ...\n",
        "    def gradient( self ):\n",
        "        #derv -- (1-gj^2(z)) on diagonal\n",
        "\n",
        "        m, n = self.getPrevIn().shape\n",
        "\n",
        "        # Calculate sigmoid derivative for each element\n",
        "        G = 1-(self.getPrevOut()**2)\n",
        "\n",
        "        # Initialize output array with proper shape (m,n,n)\n",
        "        grad_matrix = np.zeros((m, n, n))\n",
        "\n",
        "        # For each sample, create a diagonal matrix\n",
        "        for i in range(m):\n",
        "            grad_matrix[i] = np.diag(G[i,:])\n",
        "\n",
        "        return grad_matrix\n",
        "\n",
        "    def gradient2(self):\n",
        "        # Calculate sigmoid derivative for each element\n",
        "        G = 1-(self.getPrevOut()**2)\n",
        "        return G\n",
        "\n",
        "    def backward2(self,gradIn):\n",
        "        gradOut = gradIn*self.gradient2()\n",
        "        return gradOut\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSvmrUKWh02D"
      },
      "source": [
        "##Load Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bahnfcFy9Wrs"
      },
      "outputs": [],
      "source": [
        "def loadImages(fpath, folder, label, max_images):\n",
        "    \"\"\"\n",
        "    Loads images from a given directory and assigns them a label.\n",
        "    \"\"\"\n",
        "    images = []\n",
        "    labels = []\n",
        "\n",
        "    folder_path = os.path.join(fpath, folder)\n",
        "    for subfolder in ['Fake', 'Real']:\n",
        "        spath = os.path.join(folder_path, subfolder)\n",
        "        for file in os.listdir(spath):\n",
        "            img_path = os.path.join(spath, file)\n",
        "            image = Image.open(img_path).convert(\"L\")\n",
        "            image = image.resize((H,W))\n",
        "            image_array = ((np.array(image) )/ 255.0)\n",
        "            images.append(image_array)\n",
        "            labels.append(label)\n",
        "\n",
        "            # data set too large, setting limits\n",
        "            if len(images) >= max_images:\n",
        "                return images, labels\n",
        "\n",
        "    return images, labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrK2RRCqh2zM"
      },
      "source": [
        "##Run Multi-CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vXCgHjp-4NM0",
        "outputId": "96b5a97f-2647-4e08-bb4f-50f3a2ccaaf5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Begin Training\n",
            "Epoch 10, Time for last 10 epochs: 160.25s\n",
            "Train Accuracy: 0.4400\n",
            "Train Loss: 0.7708\n",
            "Epoch 20, Time for last 10 epochs: 165.19s\n",
            "Train Accuracy: 0.4400\n",
            "Train Loss: 0.7694\n"
          ]
        }
      ],
      "source": [
        "# Configuration parameters\n",
        "\n",
        "# Network parameters\n",
        "kSize1 = 3    # First convolution kernel size\n",
        "kSize2 = 5    # Second convolution kernel size\n",
        "mpSize1 = 2      # First max pooling size\n",
        "mpStride1 = 2    # First max pooling stride\n",
        "mpSize2 = 3     # Second max pooling size\n",
        "mpStride2 = 3    # Second max pooling stride\n",
        "hiddenOut = 8      # Hidden layer size\n",
        "weightOut = 1       # Output layer size (1 for binary classification)\n",
        "\n",
        "\n",
        "def multi_conv_network():\n",
        "    # Load and prepare images\n",
        "\n",
        "\n",
        "    testSet = []\n",
        "    trainSet = []\n",
        "    testLabels = []\n",
        "    trainLabels = []\n",
        "\n",
        "    testFakeImg, testFakeLabels = loadImages(fpath, 'test', 0, testFalse)\n",
        "    testRealImg, testRealLabels = loadImages(fpath, 'test', 1, testTrue)\n",
        "\n",
        "    trainFakeImg, trainFakeLabels = loadImages(fpath, 'train', 0,trainFalse)\n",
        "    trainRealImg, trainRealLabels = loadImages(fpath, 'train', 1, trainTrue)\n",
        "\n",
        "    testSet.extend(testFakeImg)\n",
        "    testSet.extend(testRealImg)\n",
        "    testLabels.extend(testFakeLabels)\n",
        "    testLabels.extend(testRealLabels)\n",
        "\n",
        "    trainSet.extend(trainFakeImg)\n",
        "    trainSet.extend(trainRealImg)\n",
        "    trainLabels.extend(trainFakeLabels)\n",
        "    trainLabels.extend(trainRealLabels)\n",
        "\n",
        "    testLabels = np.array(testLabels).reshape(-1,1)\n",
        "    trainLabels = np.array(trainLabels).reshape(-1,1)\n",
        "\n",
        "    # Create index of length of images for test and train\n",
        "    # Then shuffle, pass shuffled indices into arrays\n",
        "    testSet = np.array(testSet)\n",
        "    trainSet = np.array(trainSet)\n",
        "    testIndices = np.arange(testSet.shape[0])\n",
        "    trainIndices = np.arange(trainSet.shape[0])\n",
        "\n",
        "    testShuffle = np.random.permutation(testIndices)\n",
        "    testSet = testSet[testShuffle]\n",
        "    testLabels = testLabels[testShuffle]\n",
        "\n",
        "\n",
        "    #Dimension Calcs for FC\n",
        "    hConv1 = H - kSize1 + 1\n",
        "    wConv1 = W - kSize1 + 1\n",
        "    hPool1 = ((hConv1 - mpSize1) // mpStride1) + 1\n",
        "    wPool1 = ((wConv1 - mpSize1) // mpStride1) + 1\n",
        "    hConv2 = hPool1 - kSize2 + 1\n",
        "    wConv2 = wPool1 - kSize2 + 1\n",
        "    hPool2 = ((hConv2 - mpSize2) // mpStride2) + 1\n",
        "    wPool2 = ((wConv2 - mpSize2) // mpStride2) + 1\n",
        "    flatSize = hPool2 * wPool2\n",
        "\n",
        "    '''original structure -- reLu with maxpool ends up redundant after a few loops, right?\n",
        "\n",
        "    L1 = ConvolutionalLayer(kSize1)\n",
        "    L2 = ReLULayer()                      # --- ReLu now, maybe change up?\n",
        "    L3 = MaxPoolLayer(mpSize1, mpStride1)\n",
        "\n",
        "    L4 = ConvolutionalLayer(kSize2)\n",
        "    L5 = ReLULayer()\n",
        "    L6 = MaxPoolLayer(mpSize2, mpStride2)\n",
        "\n",
        "    L7 = FlatteningLayer()\n",
        "    L8 = FullyConnectedLayer(flatSize, hiddenOut)\n",
        "    L9 = ReLULayer()\n",
        "\n",
        "    L10 = FullyConnectedLayer(hiddenOut, weightOut)\n",
        "    L11 = LogisticSigmoidLayer()\n",
        "    L12 = LogLoss()\n",
        "    '''\n",
        "\n",
        "    L1 = ConvolutionalLayer(kSize1)\n",
        "    L2 = MaxPoolLayer(mpSize1, mpStride1)\n",
        "\n",
        "    L3 = ConvolutionalLayer(kSize2)\n",
        "    L4 = MaxPoolLayer(mpSize2, mpStride2)\n",
        "\n",
        "    L5 = FlatteningLayer()\n",
        "    L6 = FullyConnectedLayer(flatSize, hiddenOut)\n",
        "    L7 = ReLULayer()\n",
        "\n",
        "    L8 = FullyConnectedLayer(hiddenOut, weightOut)\n",
        "    L9 = LogisticSigmoidLayer()\n",
        "    L10 = LogLoss()\n",
        "\n",
        "    layers = [L1, L2, L3, L4, L5, L6, L7, L8, L9, L10]\n",
        "    loss = []\n",
        "    accuracy = []\n",
        "\n",
        "    # Train the model\n",
        "    tic = time.perf_counter()\n",
        "    print(\"Begin Training\")\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        if (epoch % 10 == 0) and (epoch != 0):\n",
        "            toc = time.perf_counter()\n",
        "            print(f\"Epoch {epoch}, Time for last 10 epochs: {toc-tic:.2f}s\")\n",
        "            print(f\"Train Accuracy: {accuracy[-1]:.4f}\")\n",
        "            print(f\"Train Loss: {loss[-1]:.4f}\")\n",
        "            tic = time.perf_counter()\n",
        "\n",
        "        # Shuffle training data for each epoch\n",
        "        trainShuffle = np.random.permutation(trainIndices)\n",
        "        trainSet = trainSet[trainShuffle]\n",
        "        trainLabels = trainLabels[trainShuffle]\n",
        "\n",
        "        # Forward pass\n",
        "        X = trainSet.copy()\n",
        "        for layer in layers[:-1]:\n",
        "            X = layer.forward(X)\n",
        "\n",
        "        # Compute loss and accuracy\n",
        "        logloss = layers[-1].eval(trainLabels, X)\n",
        "        loss.append(logloss)\n",
        "        acc = np.mean((X >= 0.5).astype(int) == trainLabels)\n",
        "        accuracy.append(acc)\n",
        "\n",
        "        # Backward pass\n",
        "        grad = layers[-1].gradient(trainLabels, X)\n",
        "\n",
        "        # Backpropagation through all layers\n",
        "        for i in range(len(layers)-2, 0, -1):\n",
        "            if isinstance(layers[i], FullyConnectedLayer):\n",
        "                layers[i].updateWeights(grad, FCLearn)\n",
        "                grad = layers[i].backward2(grad)\n",
        "            elif isinstance(layers[i], ConvolutionalLayer):\n",
        "                layers[i].updateKernels(grad, KLearn)\n",
        "                grad = layers[i].backward2(grad)\n",
        "            else:\n",
        "                grad = layers[i].backward2(grad)\n",
        "\n",
        "    print(\"\\nTraining Complete!\")\n",
        "    print(f\"Final Train Accuracy: {accuracy[-1]:.4f}\")\n",
        "    print(f\"Final Train Loss: {loss[-1]:.4f}\")\n",
        "\n",
        "    # Final evaluation on test set\n",
        "    X_test = testSet.copy()\n",
        "    for layer in layers[:-1]:\n",
        "        X_test = layer.forward(X_test)\n",
        "\n",
        "    test_logloss = layers[-1].eval(testLabels, X_test)\n",
        "    test_acc = np.mean((X_test >= 0.5).astype(int) == testLabels)\n",
        "\n",
        "    print(f\"Final Test Accuracy: {test_acc:.4f}\")\n",
        "    print(f\"Final Test Loss: {test_logloss:.4f}\")\n",
        "\n",
        "    # Plot training and validation metrics\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(range(epochs), loss, label=\"Train Loss\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Log Loss\")\n",
        "    plt.title(\"Loss over Epochs\")\n",
        "    plt.legend()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Run the model\n",
        "if __name__ == \"__main__\":\n",
        "    multi_conv_network()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "zMFqrJrFBdUp",
        "MkHOriCZceeR",
        "4Zd0R8k_iMjr",
        "ydtQCwDNajMJ",
        "BNP3URqVjK42",
        "inlck7g6Jp60",
        "W0F8k6aKiv_G",
        "19Khpy4eJxox",
        "_24f_hdWi4Zr",
        "djrXMwfHi6wF",
        "dzptlFt5hqf1",
        "KSvmrUKWh02D"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}